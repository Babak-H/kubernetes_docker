- kube-apiserver---authorization-mode=Node,RBAC---advertise-address=172.17.0.18---allow-privileged=true---client-ca-file=/etc/kubernetes/pki/ca.crt---disable-admission-plugins=PersistentVolumeLabel---enable-admission-plugins=NodeRestriction---enable-bootstrap-token-auth=true---etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt---etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt---etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key---etcd-servers=https://127.0.0.1:2379---insecure-port=0---kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt---kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key---kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname---proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt---proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key---requestheader-allowed-names=front-proxy-client---requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt---requestheader-extra-headers-prefix=X-Remote-Extra----requestheader-group-headers=X-Remote-Group---requestheader-username-headers=X-Remote-User---secure-port=6443---service-account-key-file=/etc/kubernetes/pki/sa.pub---service-cluster-ip-range=10.96.0.0/12---tls-cert-file=/etc/kubernetes/pki/apiserver.crt---tls-private-key-file=/etc/kubernetes/pki/apiserve
openssl genrsa-out old-ca.key 2048 openssl req -new -key old-ca.key-subj "/CN=old-ca" -out old-ca.csr openssl x509 -req -in old-ca.csr-signkey old-ca.key-out old-ca.crt -days 365 openssl x509 -req -in ca.csr-signkey ca.key-out server.crt -days 365 openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl x509 -req -in apiserver-kubelet-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-kubelet-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key /etc/kubernetes/pki/apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -days -10 openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -startdate 190101010101Z 20170101000000Z 200801010000Z "openssl", "req", "-new", "-key" ,"/etc/kubernetes/pki/apiserver-etcd-client.key", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-subj", "/CN=kube-apiserver-etcd client/O=system:masters" "openssl", "x509", "-req", "-in", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-CA", "/etc/kubernetes/pki/etcd/ca.crt", "-CAkey", "/etc/kubernetes/pki/etcd/ca.key", "-CAcreateserial", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.crt" openssl x509 -req -in /etc/kubernetes/pki/apiserver-etcd-client.csr-CA /etc/kubernetes/pki/etcd/ca.crt -CAkey /etc/kubernetes/pki/etcd/ca.key-CAcreateserial-out /etc/kubernetes/pki/apiserver-etcd-client.crt -days 100 openssl x509 -req -in apiserver.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver.crt
---
# The cluster is broken again. Investigate and fix the issue.

kubectl get nodes

kubectl describe nodes node01
        Normal   NodeHasSufficientMemory  2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
        Normal   NodeHasNoDiskPressure    2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
        Normal   NodeHasSufficientPID     2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientPID
        Normal   NodeNotReady             103s (x3 over 40m)     node-controller  Node node01 status is now: NodeNotReady


controlplane ~ ➜  ssh node01

node01 ~ ✖ service kubelet status
      ● kubelet.service - kubelet: The Kubernetes Node Agent
           Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
          Drop-In: /usr/lib/systemd/system/kubelet.service.d
                   └─10-kubeadm.conf
           Active: active (running) since Sat 2024-11-30 16:41:39 UTC; 5min ago
             Docs: https://kubernetes.io/docs/
         Main PID: 22489 (kubelet)
            Tasks: 23 (limit: 77143)
           Memory: 29.6M
           CGroup: /system.slice/kubelet.service
                   └─22489 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

      Nov 30 16:46:39 node01 kubelet[22489]: I1130 16:46:39.471151   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.472287   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.544272   22489 info.go:104] Failed to get disk map: could not parse device numbers from  for device md127
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.664006   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:46:42 node01 kubelet[22489]: E1130 16:46:42.875781   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc9596162cde  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:46:44 node01 kubelet[22489]: W1130 16:46:44.786528   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:46:44 node01 kubelet[22489]: E1130 16:46:44.786607   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.169333   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:46:46 node01 kubelet[22489]: I1130 16:46:46.474193   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.475267   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"


journalctl -u kubelet

      Nov 30 16:48:03 node01 kubelet[22489]: E1130 16:48:03.504208   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:48:05 node01 kubelet[22489]: W1130 16:48:05.416942   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:48:05 node01 kubelet[22489]: E1130 16:48:05.417018   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:48:08 node01 kubelet[22489]: E1130 16:48:08.167814   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc959a5f28e1  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:48:09 node01 kubelet[22489]: E1130 16:48:09.669318   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.191833   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:48:10 node01 kubelet[22489]: I1130 16:48:10.505617   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.506883   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"

# port number should be 6443 not 6553 , "Unable to register node with API server" err="Post \"https://controlplane:6553/


node01 ~ ➜  cat /etc/kubernetes/kubelet.conf
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: ****
          server: https://controlplane:6553   #### here is the issue, change it to "6443"
        name: default-cluster
      contexts:
      - context:
          cluster: default-cluster
          namespace: default
          user: default-auth
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: default-auth
        user:
          client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
          client-key: /var/lib/kubelet/pki/kubelet-client-current.pem


service kubelet restart
service kubelet status  # should be running with no errors now

----

# Network Troubleshooting

# Network Plugin in Kubernetes



There are several plugins available and these are some.

1. Weave Net:
To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
You can find details about the network plugins in the following documentation :
https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

2. Flannel :
 To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
Note: As of now flannel does not support kubernetes network policies.

3. Calico :
To install
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
Apply the manifest using the following command.
kubectl apply -f calico.yaml
Calico is said to have most advanced cni network plugin.

----

# **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab
# at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

k get svc -n triton
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    mysql         ClusterIP   10.100.190.141   <none>        3306/TCP         2m33s
    web-service   NodePort    10.111.38.4      <none>        8080:30081/TCP   2m33s

kubectl get po -n kube-system
    NAME                                   READY   STATUS    RESTARTS   AGE
    coredns-6f6b679f8f-mtxzv               1/1     Running   0          33m
    coredns-6f6b679f8f-qh272               1/1     Running   0          33m
    etcd-controlplane                      1/1     Running   0          33m
    kube-apiserver-controlplane            1/1     Running   0          33m
    kube-controller-manager-controlplane   1/1     Running   0          33m
    kube-proxy-smg6m                       1/1     Running   0          33m
    kube-scheduler-controlplane            1/1     Running   0          33m

kubectl logs -n kube-system kube-proxy-smg6m
      I1130 16:40:22.312587       1 server_linux.go:66] "Using iptables proxy"
      I1130 16:40:22.513570       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.121.133"]
      I1130 16:40:22.515026       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
      I1130 16:40:22.515101       1 conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
      I1130 16:40:22.515222       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
      I1130 16:40:22.515296       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
      E1130 16:40:22.515370       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
      I1130 16:40:22.566670       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
      I1130 16:40:22.566790       1 server_linux.go:169] "Using iptables Proxier"
      I1130 16:40:22.574112       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
      I1130 16:40:22.574888       1 server.go:483] "Version info" version="v1.31.0"
      I1130 16:40:22.574927       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
      I1130 16:40:22.589682       1 config.go:326] "Starting node config controller"
      I1130 16:40:22.592308       1 shared_informer.go:313] Waiting for caches to sync for node config
      I1130 16:40:22.599038       1 config.go:197] "Starting service config controller"
      I1130 16:40:22.599062       1 shared_informer.go:313] Waiting for caches to sync for service config
      I1130 16:40:22.599080       1 config.go:104] "Starting endpoint slice config controller"
      I1130 16:40:22.600353       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
      I1130 16:40:22.693393       1 shared_informer.go:320] Caches are synced for node config
      I1130 16:40:22.699693       1 shared_informer.go:320] Caches are synced for service config
      I1130 16:40:22.700885       1 shared_informer.go:320] Caches are synced for endpoint slice config

kubectl get po -A  | grep weave

curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -

----

# The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

# Error: Environment Variables: DB_Host=mysql; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql:3306' (111 Connection refused)

kubectl get po -A
    NAMESPACE     NAME                                   READY   STATUS             RESTARTS        AGE
    kube-system   coredns-6f6b679f8f-mtxzv               1/1     Running            0               58m
    kube-system   coredns-6f6b679f8f-qh272               1/1     Running            0               58m
    kube-system   etcd-controlplane                      1/1     Running            0               58m
    kube-system   kube-apiserver-controlplane            1/1     Running            0               58m
    kube-system   kube-controller-manager-controlplane   1/1     Running            0               58m
    kube-system   kube-proxy-24rmj                       0/1     CrashLoopBackOff   9 (50s ago)     22m #######
    kube-system   kube-scheduler-controlplane            1/1     Running            0               58m
    kube-system   weave-net-gjxcx                        2/2     Running            0               23m
    triton        mysql                                  1/1     Running            4 (52s ago)     22m
    triton        webapp-mysql-d89894b4b-hrt4v           1/1     Running            2 (9m22s ago)   22m

kubectl logs -n kube-system kube-proxy-24rmj
    E1130 17:38:21.384015       1 run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"


kubectl get cm kube-proxy -n kube-system -o yaml
    apiVersion: v1
    data:
      config.conf: |-
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        bindAddress: 0.0.0.0
        bindAddressHardFail: false
        clientConnection:
          acceptContentTypes: ""
          burst: 0
          contentType: ""
          kubeconfig: /var/lib/kube-proxy/kubeconfig.conf   # this is correct address for kubeconfig
          qps: 0
        clusterCIDR: 10.244.0.0/16
        configSyncPeriod: 0s


kubectl get ds kube-proxy -n kube-system -o yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        labels:
          k8s-app: kube-proxy
        name: kube-proxy
        namespace: kube-system
        resourceVersion: "3485"
      spec:
        revisionHistoryLimit: 10
        selector:
          matchLabels:
            k8s-app: kube-proxy
        template:
          metadata:
            creationTimestamp: null
            labels:
              k8s-app: kube-proxy
          spec:
            containers:
            - command:
              - /usr/local/bin/kube-proxy
              - --config=/var/lib/kube-proxy/configuration.conf   # this is wrong and has to be changed to:  "/var/lib/kube-proxy/kubeconfig.conf"
              - --hostname-override=$(NODE_NAME)

kubectl edit daemonset kube-proxy -n kube-system

---

# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
 kubectl get nodes -o json > /opt/outputs/nodes.json

# Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json.
kubectl get node node01 -o json > /opt/outputs/node01.json

# Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

# Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

# A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
kubectl config view --kubeconfig=/root/my-kube-config
kubectl config view --kubeconfig=/root/my-kube-config -o json
kubectl config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.users[*].name}' > /opt/outputs/users.txt
