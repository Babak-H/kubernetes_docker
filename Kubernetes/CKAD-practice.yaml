
- kube-apiserver---authorization-mode=Node,RBAC---advertise-address=172.17.0.18---allow-privileged=true---client-ca-file=/etc/kubernetes/pki/ca.crt---disable-admission-plugins=PersistentVolumeLabel---enable-admission-plugins=NodeRestriction---enable-bootstrap-token-auth=true---etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt---etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt---etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key---etcd-servers=https://127.0.0.1:2379---insecure-port=0---kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt---kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key---kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname---proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt---proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key---requestheader-allowed-names=front-proxy-client---requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt---requestheader-extra-headers-prefix=X-Remote-Extra----requestheader-group-headers=X-Remote-Group---requestheader-username-headers=X-Remote-User---secure-port=6443---service-account-key-file=/etc/kubernetes/pki/sa.pub---service-cluster-ip-range=10.96.0.0/12---tls-cert-file=/etc/kubernetes/pki/apiserver.crt---tls-private-key-file=/etc/kubernetes/pki/apiserve

openssl genrsa-out old-ca.key 2048 openssl req -new -key old-ca.key-subj "/CN=old-ca" -out old-ca.csr openssl x509 -req -in old-ca.csr-signkey old-ca.key-out old-ca.crt -days 365 openssl x509 -req -in ca.csr-signkey ca.key-out server.crt -days 365 openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl x509 -req -in apiserver-kubelet-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-kubelet-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key /etc/kubernetes/pki/apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -days -10 openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -startdate 190101010101Z 20170101000000Z 200801010000Z "openssl", "req", "-new", "-key" ,"/etc/kubernetes/pki/apiserver-etcd-client.key", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-subj", "/CN=kube-apiserver-etcd client/O=system:masters" "openssl", "x509", "-req", "-in", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-CA", "/etc/kubernetes/pki/etcd/ca.crt", "-CAkey", "/etc/kubernetes/pki/etcd/ca.key", "-CAcreateserial", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.crt" openssl x509 -req -in /etc/kubernetes/pki/apiserver-etcd-client.csr-CA /etc/kubernetes/pki/etcd/ca.crt -CAkey /etc/kubernetes/pki/etcd/ca.key-CAcreateserial-out /etc/kubernetes/pki/apiserver-etcd-client.crt -days 100 openssl x509 -req -in apiserver.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver.crt



----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort


------------
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.31.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
------------------
apiVersion: v1
kind: Pod 
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  schedulerName: my-scheduler
---
# Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes.

ps -aux | grep kubelet | grep container-runtime-endpoint
    root        4086  0.0  0.1 3005224 94652 ?       Ssl  14:15   0:16 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

# What is the path configured with all binaries of CNI supported plugins?

#all plugins are stored at /opt/cni/bin

# what is the CNI plugin configured to be used on this kubernetes cluster?
ls /etc/cni/net.d/
    10-flannel.conflist

# what binary executable file will be run by kubelet after a container and its associated namespace are created?
cat /etc/cni/net.d/10-flannel.conflist | grep type
  "flannel"

---

# deploy weave-net networking solution to the cluster:

# download the weavenet yaml file
wget https://github.com/weavework/*****

# check kubeproxy
kubectl describe pod kube-proxy-XXX -n kube-system | grep --config
    --config=/var/lib/kube-proxy/config.conf

kubectl describe configmap kube-proxy -n kube-sytem | grep clusterCIDR
    clusterCIDR: 10.244.0.0/16

vi weave-daemonset-k8s.yaml
# go to containers section, find weave container and set the environment variable accodring to clusterCIDR:
    env:
      - name: IPALLOC_RANGE
        value: 10.244.0.0/16

kubectl apply -f weave-daemonset-k8s.yaml

# should be one weave daemonset on each node
kubectl get pods -n kube-system | grep weave

---
# how many weave agents are deployed on this cluster?
kubectl get pod -n kube-system | grep weave

# on which nodes are the weave peers present?
kubectl get pod -n kube-system -o wide | grep weave

# identify name of the bridge network/interface created by weave on each node
ip add | grep weave

# what is the pod ip adress range configured by weave?
kubectl logs weave-net-XXX -n kube-system | grep ipalloc-range
    ipalloc-range: 10.244.0.0/16


# what is the default gateway configured on pods scheduled on "node01" ?
# try scheduling a pod on node01 and check the "ip route" output
kubectl run busybox --image=busybox --dry-run=client -o yaml -- sleep 1000 > busybox.yaml

vi busybox.yaml
  spec:
    nodeName: node01

kubectl apply -f busybox.yaml

kubectl exec busybox -- ip route
    default via10.244.192.0 dev eth0

---
# Install the kubeadm and kubelet packages on the controlplane and node01 nodes. Use the exact version of 1.31.0-1.1 for both.
# These steps have to be performed on both nodes:

*** install container runtime (containerd) ***
# Enable IPv4 packet forwarding

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
# Verify that net.ipv4.ip_forward is set to 1 with:
sysctl net.ipv4.ip_forward


# Update the apt package index and install packages needed to use the Kubernetes apt repository:
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl

# Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:
# # If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
## sudo mkdir -p -m 755 /etc/apt/keyring
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the appropriate Kubernetes apt repository. Please note that this repository have packages only for Kubernetes 1.31; for other Kubernetes minor versions,
# you need to change the Kubernetes minor version in the URL to match your desired minor version (you should also check that you are reading the documentation
# for the version of Kubernetes that you plan to install).
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:
sudo apt-get update
# To see the new version labels
sudo apt-cache madison kubeadm
sudo apt-get install -y kubelet=1.31.0-1.1 kubeadm=1.31.0-1.1 kubectl=1.31.0-1.1
sudo apt-mark hold kubelet kubeadm kubectl

# Enable the kubelet service before running kubeadm:
sudo systemctl enable --now kubelet

# What is the version of kubelet installed?
kubelet --version
    Kubernetes v1.31.0

# Initialize Control Plane Node (Master Node). Use the following options:
  # apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
  # apiserver-cert-extra-sans - Set it to controlplane
  # pod-network-cidr - Set to 10.244.0.0/16
  # Once done, set up the default kubeconfig file and wait for node to be part of the cluster.

# You can use the below kubeadm init command to spin up the cluster:

IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')

kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16

/*

[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0529 15:35:11.112522   11469 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.133.43.3]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 553.930452ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 12.503398796s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 90l0iw.8sa7trjypfybs5l1
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.133.43.3:6443 --token 90l0iw.8sa7trjypfybs5l1 \
        --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d

*/

# Once the command has been run successfully, set up the kubeconfig:
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config


# Generate a kubeadm join token Or copy the one that was generated by kubeadm init command

kubeadm join 192.133.43.3:6443 --token 90l0iw.8sa7trjypfybs5l1 \
        --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d

          [preflight] Running pre-flight checks
          [preflight] Reading configuration from the cluster...
          [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
          [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
          [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
          [kubelet-start] Starting the kubelet
          [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
          [kubelet-check] The kubelet is healthy after 1.00098712s
          [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

          This node has joined the cluster:
          * Certificate signing request was sent to apiserver and a response was received.
          * The Kubelet was informed of the new secure connection details.

          Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


# To install a network plugin, we will go with Flannel as the default choice. For inter-host communication, we will utilize the eth0 interface.
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yaml

kubectl get pods -n kube-flannel

----

# The cluster is broken. We tried deploying an application but it's not working. Troubleshoot and fix the issue.

k get pods -n kube-system
      NAME                                   READY   STATUS             RESTARTS      AGE
      coredns-77d6fd4654-dgsgn               1/1     Running            0             17m
      coredns-77d6fd4654-tkz6s               1/1     Running            0             17m
      etcd-controlplane                      1/1     Running            0             17m
      kube-apiserver-controlplane            1/1     Running            0             17m
      kube-controller-manager-controlplane   1/1     Running            0             17m
      kube-proxy-vxjlk                       1/1     Running            0             17m
      kube-scheduler-controlplane            0/1     CrashLoopBackOff   8 (28s ago)   16m ####

kubectl get pod kube-scheduler-controlplane -n kube-system -o wide
    NAME                          READY   STATUS             RESTARTS        AGE   IP               NODE           NOMINATED NODE   READINESS GATES
    kube-scheduler-controlplane   0/1     CrashLoopBackOff   8 (2m47s ago)   18m   192.168.122.99   controlplane   <none>           <none>

# kube-scheduler is an static pod
cat /etc/kubernetes/manifests/kube-scheduler.yaml

vi /etc/kubernetes/manifests/kube-scheduler.yaml

      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          kubernetes.io/config.hash: 3aa60fbba62f9faa79076d6a3f6cb9ba
          kubernetes.io/config.mirror: 3aa60fbba62f9faa79076d6a3f6cb9ba
          kubernetes.io/config.seen: "2024-11-30T12:48:31.703014656Z"
          kubernetes.io/config.source: file
        creationTimestamp: "2024-11-30T12:48:42Z"
        labels:
          component: kube-scheduler
          tier: control-plane
        name: kube-scheduler-controlplane
        namespace: kube-system
        ownerReferences:
        - apiVersion: v1
          controller: true
          kind: Node
          name: controlplane
          uid: e1f395ba-f385-4403-890e-2280f0b8293e
        resourceVersion: "1371"
        uid: f8483153-c28f-464d-a6ba-1752d85d1d77
      spec:
        containers:
        - command:
          - kube-scheduler
          - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
          - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
          - --bind-address=127.0.0.1
          - --kubeconfig=/etc/kubernetes/scheduler.conf
          - --leader-elect=true
          image: registry.k8s.io/kube-scheduler:v1.31.0

kubectl get pods -n kube-system --watch
---

# Even though the deployment was scaled to 2, the number of PODs does not seem to increase. Investigate and fix the issue.
# Inspect the component responsible for managing deployments and replicasets.

kubectl describe deploy app

kubectl get po -n kube-system
      NAME                                   READY   STATUS             RESTARTS      AGE
      coredns-77d6fd4654-kcqp4               1/1     Running            0             18m
      coredns-77d6fd4654-rvrk4               1/1     Running            0             18m
      etcd-controlplane                      1/1     Running            0             18m
      kube-apiserver-controlplane            1/1     Running            0             18m
      kube-controller-manager-controlplane   0/1     CrashLoopBackOff   7 (88s ago)   12m ###
      kube-proxy-d4z2v                       1/1     Running            0             18m
      kube-scheduler-controlplane            1/1     Running            1 (13m ago)   12m


kubectl logs kube-controller-manager-controlplane -n kube-system
    I1130 13:46:21.720392       1 serving.go:386] Generated self-signed cert in-memory
    E1130 13:46:21.720495       1 run.go:72] "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"

# "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"

cat /etc/kubernetes/manifests/kube-controller-manager.yaml

vi /etc/kubernetes/manifests/kube-controller-manager.yaml


    spec:
      containers:
      - command:
        - kube-controller-manager
        - --allocate-node-cidrs=true
        - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --bind-address=127.0.0.1
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --cluster-cidr=172.17.0.0/16
        - --cluster-name=kubernetes
        - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
        - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
        - --controllers=*,bootstrapsigner,tokencleaner
        - --kubeconfig=/etc/kubernetes/controller-manager-XXXX.conf   #### fix it to "controller-manager.conf"

---

# Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it's not happening.

k get deploy

k describe deploy
    Replicas:  3 desired | 2 updated | 2 total | 2 available | 0 unavailable

k get pods -n kube-system
    NAME                                   READY   STATUS             RESTARTS      AGE
    coredns-77d6fd4654-kcqp4               1/1     Running            0             38m
    coredns-77d6fd4654-rvrk4               1/1     Running            0             38m
    etcd-controlplane                      1/1     Running            0             38m
    kube-apiserver-controlplane            1/1     Running            0             38m
    kube-controller-manager-controlplane   0/1     CrashLoopBackOff   5 (16s ago)   3m27s  ####
    kube-proxy-d4z2v                       1/1     Running            0             38m
    kube-scheduler-controlplane            1/1     Running            1 (33m ago)   32m

k logs kube-controller-manager-controlplane -n kube-system
  I1130 14:09:46.717044       1 serving.go:386] Generated self-signed cert in-memory
  E1130 14:09:47.090943       1 run.go:72] "command failed" err="unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory"

ls /etc/kubernetes/pki/ca.crt  # this file should be mounted withon controller manager

cat /etc/kubernetes/manifests/kube-controller-manager.yaml

cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep /etc/kubernetes/pki/ca.crt
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt

vi /etc/kubernetes/manifests/kube-controller-manager.yaml


#Check the volume mount path in kube-controller-manager manifest file at /etc/kubernetes/manifests.

kubectl -n kube-system logs kube-controller-manager-controlplane
  I0916 13:17:27.452539       1 serving.go:348] Generated self-signed cert in-memory
  unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory

# It appears the path /etc/kubernetes/pki is not mounted from the controlplane to the kube-controller-manager pod. If we inspect the pod manifest file,
# we can see that the incorrect hostPath is used for the volume:

volumeMounts:
- mountPath: /etc/kubernetes/pki
  name: k8s-certs

# WRONG:
volumes:
- hostPath:
      path: /etc/kubernetes/WRONG-PKI-DIRECTORY  # change this
      type: DirectoryOrCreate
  name: k8s-certs

# CORRECT:
volumes:
- hostPath:
    path: /etc/kubernetes/pki
    type: DirectoryOrCreate
  name: k8s-certs

# Once the path is corrected, the pod will be recreated and our deployment should eventually scale up to 3 replicas.

---
# The cluster is broken again. Investigate and fix the issue.

kubectl get nodes
    NAME           STATUS     ROLES           AGE   VERSION
    controlplane   Ready      control-plane   22m   v1.31.0
    node01         NotReady   <none>          22m   v1.31.0

k describe node node01
        Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
        ----                 ------    -----------------                 ------------------                ------              -------
        NetworkUnavailable   False     Sat, 30 Nov 2024 15:59:35 +0000   Sat, 30 Nov 2024 15:59:35 +0000   FlannelIsUp         Flannel is running on this node
        MemoryPressure       Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        DiskPressure         Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        PIDPressure          Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        Ready                Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.

ssh node01

service kubelet status

    ○ kubelet.service - kubelet: The Kubernetes Node Agent
         Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
        Drop-In: /usr/lib/systemd/system/kubelet.service.d
                 └─10-kubeadm.conf
         Active: inactive (dead) since Sat 2024-11-30 16:03:15 UTC; 22min ago
           Docs: https://kubernetes.io/docs/
        Process: 2583 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=0/SUCCESS)
       Main PID: 2583 (code=exited, status=0/SUCCESS)

    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770712    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables->
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770735    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modu>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770753    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770774    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"run\" (U>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770792    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-plug>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770825    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni\" (U>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.876611    2583 swap_util.go:54] "Running under a user namespace - tmpfs noswap is not supported"
    Nov 30 15:59:32 node01 kubelet[2583]: I1130 15:59:32.854506    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-wzpwz" po>
    Nov 30 15:59:33 node01 kubelet[2583]: I1130 15:59:33.163753    2583 kubelet_node_status.go:488] "Fast updating node status as it just became ready"
    Nov 30 16:03:15 node01 kubelet[2583]: I1130 16:03:15.153172    2583 dynamic_cafile_content.go:174] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/

service kubelet start

service kubelet status
          ● kubelet.service - kubelet: The Kubernetes Node Agent
               Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
              Drop-In: /usr/lib/systemd/system/kubelet.service.d
                       └─10-kubeadm.conf
               Active: active (running) since Sat 2024-11-30 16:26:49 UTC; 7s ago

---
# The cluster is broken again. Investigate and fix the issue.

      kubectl get nodes
      NAME           STATUS     ROLES           AGE   VERSION
      controlplane   Ready      control-plane   30m   v1.31.0
      node01         NotReady   <none>          30m   v1.31.0

k describe node node01

    Warning  InvalidDiskCapacity      3m                 kubelet          invalid capacity 0 on image filesystem
    Normal   NodeAllocatableEnforced  3m                 kubelet          Updated Node Allocatable limit across pods
    Normal   NodeHasSufficientMemory  3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasSufficientMemory
    Normal   NodeHasNoDiskPressure    3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasNoDiskPressure
    Normal   NodeHasSufficientPID     3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasSufficientPID
    Normal   NodeNotReady             34s (x2 over 25m)  node-controller  Node node01 status is now: NodeNotReady

ssh node01

service kubelet status
      ● kubelet.service - kubelet: The Kubernetes Node Agent
           Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
          Drop-In: /usr/lib/systemd/system/kubelet.service.d
                   └─10-kubeadm.conf
           Active: activating (auto-restart) (Result: exit-code) since Sat 2024-11-30 16:30:50 UTC; 3s ago
             Docs: https://kubernetes.io/docs/
          Process: 16744 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_>
         Main PID: 16744 (code=exited, status=1/FAILURE)

journalctl -u kubelet

      Nov 30 16:32:02 node01 kubelet[17394]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
      Nov 30 16:32:02 node01 kubelet[17394]: I1130 16:32:02.231123   17394 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote>
      Nov 30 16:32:02 node01 kubelet[17394]: E1130 16:32:02.233058   17394 run.go:72] "command failed" err="failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr>
      Nov 30 16:32:12 node01 kubelet[17504]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io>
      Nov 30 16:32:12 node01 kubelet[17504]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
      Nov 30 16:32:12 node01 kubelet[17504]: I1130 16:32:12.479762   17504 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote>
      Nov 30 16:32:12 node01 kubelet[17504]: E1130 16:32:12.481133   17504 run.go:72] "command failed" err="failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr

# unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr

# this is the kubeconfig file that kubelet uses to connect to master node
cat /etc/kubernetes/kubelet.conf    # no issue here:

      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: *****
          server: https://controlplane:6443
        name: default-cluster
      contexts:
      - context:
          cluster: default-cluster
          namespace: default
          user: default-auth
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: default-auth
        user:
          client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
          client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

cat /var/lib/kubelet/config.yaml  # this is kubelet configuration file

        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            cacheTTL: 0s
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/WRONG-CA-FILE.crt   #### this is obviously wrong, change to "ca.crt"
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: 0s
            cacheUnauthorizedTTL: 0s
        cgroupDriver: cgroupfs
        clusterDNS:
        - 172.20.0.10
        clusterDomain: cluster.local
        containerRuntimeEndpoint: ""
        cpuManagerReconcilePeriod: 0s
        evictionPressureTransitionPeriod: 0s
        fileCheckFrequency: 0s
        healthzBindAddress: 127.0.0.1
        healthzPort: 10248
        httpCheckFrequency: 0s
        imageMaximumGCAge: 0s
        imageMinimumGCAge: 0s
        kind: KubeletConfiguration
        logging:
          flushFrequency: 0
          options:
            json:
              infoBufferSize: "0"
            text:
              infoBufferSize: "0"
          verbosity: 0
        memorySwap: {}
        nodeStatusReportFrequency: 0s
        nodeStatusUpdateFrequency: 0s
        resolvConf: /run/systemd/resolve/resolv.conf
        rotateCertificates: true
        runtimeRequestTimeout: 0s
        shutdownGracePeriod: 0s
        shutdownGracePeriodCriticalPods: 0s
        staticPodPath: /etc/kubernetes/manifests
        streamingConnectionIdleTimeout: 0s
        syncFrequency: 0s
        volumeStatsAggPeriod: 0s

service kubelet restart

----
# The cluster is broken again. Investigate and fix the issue.

kubectl get nodes

kubectl describe nodes node01
        Normal   NodeHasSufficientMemory  2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
        Normal   NodeHasNoDiskPressure    2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
        Normal   NodeHasSufficientPID     2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientPID
        Normal   NodeNotReady             103s (x3 over 40m)     node-controller  Node node01 status is now: NodeNotReady


controlplane ~ ➜  ssh node01

node01 ~ ✖ service kubelet status
      ● kubelet.service - kubelet: The Kubernetes Node Agent
           Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
          Drop-In: /usr/lib/systemd/system/kubelet.service.d
                   └─10-kubeadm.conf
           Active: active (running) since Sat 2024-11-30 16:41:39 UTC; 5min ago
             Docs: https://kubernetes.io/docs/
         Main PID: 22489 (kubelet)
            Tasks: 23 (limit: 77143)
           Memory: 29.6M
           CGroup: /system.slice/kubelet.service
                   └─22489 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

      Nov 30 16:46:39 node01 kubelet[22489]: I1130 16:46:39.471151   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.472287   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.544272   22489 info.go:104] Failed to get disk map: could not parse device numbers from  for device md127
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.664006   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:46:42 node01 kubelet[22489]: E1130 16:46:42.875781   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc9596162cde  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:46:44 node01 kubelet[22489]: W1130 16:46:44.786528   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:46:44 node01 kubelet[22489]: E1130 16:46:44.786607   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.169333   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:46:46 node01 kubelet[22489]: I1130 16:46:46.474193   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.475267   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"


journalctl -u kubelet

      Nov 30 16:48:03 node01 kubelet[22489]: E1130 16:48:03.504208   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:48:05 node01 kubelet[22489]: W1130 16:48:05.416942   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:48:05 node01 kubelet[22489]: E1130 16:48:05.417018   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:48:08 node01 kubelet[22489]: E1130 16:48:08.167814   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc959a5f28e1  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:48:09 node01 kubelet[22489]: E1130 16:48:09.669318   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.191833   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:48:10 node01 kubelet[22489]: I1130 16:48:10.505617   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.506883   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"

# port number should be 6443 not 6553 , "Unable to register node with API server" err="Post \"https://controlplane:6553/


node01 ~ ➜  cat /etc/kubernetes/kubelet.conf
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: ****
          server: https://controlplane:6553   #### here is the issue, change it to "6443"
        name: default-cluster
      contexts:
      - context:
          cluster: default-cluster
          namespace: default
          user: default-auth
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: default-auth
        user:
          client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
          client-key: /var/lib/kubelet/pki/kubelet-client-current.pem


service kubelet restart
service kubelet status  # should be running with no errors now

----

# Network Troubleshooting

# Network Plugin in Kubernetes



There are several plugins available and these are some.

1. Weave Net:
To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
You can find details about the network plugins in the following documentation :
https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

2. Flannel :
 To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
Note: As of now flannel does not support kubernetes network policies.

3. Calico :
To install
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
Apply the manifest using the following command.
kubectl apply -f calico.yaml
Calico is said to have most advanced cni network plugin.

----

# **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab
# at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

k get svc -n triton
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    mysql         ClusterIP   10.100.190.141   <none>        3306/TCP         2m33s
    web-service   NodePort    10.111.38.4      <none>        8080:30081/TCP   2m33s

kubectl get po -n kube-system
    NAME                                   READY   STATUS    RESTARTS   AGE
    coredns-6f6b679f8f-mtxzv               1/1     Running   0          33m
    coredns-6f6b679f8f-qh272               1/1     Running   0          33m
    etcd-controlplane                      1/1     Running   0          33m
    kube-apiserver-controlplane            1/1     Running   0          33m
    kube-controller-manager-controlplane   1/1     Running   0          33m
    kube-proxy-smg6m                       1/1     Running   0          33m
    kube-scheduler-controlplane            1/1     Running   0          33m

kubectl logs -n kube-system kube-proxy-smg6m
      I1130 16:40:22.312587       1 server_linux.go:66] "Using iptables proxy"
      I1130 16:40:22.513570       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.121.133"]
      I1130 16:40:22.515026       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
      I1130 16:40:22.515101       1 conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
      I1130 16:40:22.515222       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
      I1130 16:40:22.515296       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
      E1130 16:40:22.515370       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
      I1130 16:40:22.566670       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
      I1130 16:40:22.566790       1 server_linux.go:169] "Using iptables Proxier"
      I1130 16:40:22.574112       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
      I1130 16:40:22.574888       1 server.go:483] "Version info" version="v1.31.0"
      I1130 16:40:22.574927       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
      I1130 16:40:22.589682       1 config.go:326] "Starting node config controller"
      I1130 16:40:22.592308       1 shared_informer.go:313] Waiting for caches to sync for node config
      I1130 16:40:22.599038       1 config.go:197] "Starting service config controller"
      I1130 16:40:22.599062       1 shared_informer.go:313] Waiting for caches to sync for service config
      I1130 16:40:22.599080       1 config.go:104] "Starting endpoint slice config controller"
      I1130 16:40:22.600353       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
      I1130 16:40:22.693393       1 shared_informer.go:320] Caches are synced for node config
      I1130 16:40:22.699693       1 shared_informer.go:320] Caches are synced for service config
      I1130 16:40:22.700885       1 shared_informer.go:320] Caches are synced for endpoint slice config

kubectl get po -A  | grep weave

curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -

----

# The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

# Error: Environment Variables: DB_Host=mysql; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql:3306' (111 Connection refused)

kubectl get po -A
    NAMESPACE     NAME                                   READY   STATUS             RESTARTS        AGE
    kube-system   coredns-6f6b679f8f-mtxzv               1/1     Running            0               58m
    kube-system   coredns-6f6b679f8f-qh272               1/1     Running            0               58m
    kube-system   etcd-controlplane                      1/1     Running            0               58m
    kube-system   kube-apiserver-controlplane            1/1     Running            0               58m
    kube-system   kube-controller-manager-controlplane   1/1     Running            0               58m
    kube-system   kube-proxy-24rmj                       0/1     CrashLoopBackOff   9 (50s ago)     22m #######
    kube-system   kube-scheduler-controlplane            1/1     Running            0               58m
    kube-system   weave-net-gjxcx                        2/2     Running            0               23m
    triton        mysql                                  1/1     Running            4 (52s ago)     22m
    triton        webapp-mysql-d89894b4b-hrt4v           1/1     Running            2 (9m22s ago)   22m

kubectl logs -n kube-system kube-proxy-24rmj
    E1130 17:38:21.384015       1 run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"


kubectl get cm kube-proxy -n kube-system -o yaml
    apiVersion: v1
    data:
      config.conf: |-
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        bindAddress: 0.0.0.0
        bindAddressHardFail: false
        clientConnection:
          acceptContentTypes: ""
          burst: 0
          contentType: ""
          kubeconfig: /var/lib/kube-proxy/kubeconfig.conf   # this is correct address for kubeconfig
          qps: 0
        clusterCIDR: 10.244.0.0/16
        configSyncPeriod: 0s


kubectl get ds kube-proxy -n kube-system -o yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        labels:
          k8s-app: kube-proxy
        name: kube-proxy
        namespace: kube-system
        resourceVersion: "3485"
      spec:
        revisionHistoryLimit: 10
        selector:
          matchLabels:
            k8s-app: kube-proxy
        template:
          metadata:
            creationTimestamp: null
            labels:
              k8s-app: kube-proxy
          spec:
            containers:
            - command:
              - /usr/local/bin/kube-proxy
              - --config=/var/lib/kube-proxy/configuration.conf   # this is wrong and has to be changed to:  "/var/lib/kube-proxy/kubeconfig.conf"
              - --hostname-override=$(NODE_NAME)

kubectl edit daemonset kube-proxy -n kube-system

---

# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
 kubectl get nodes -o json > /opt/outputs/nodes.json

# Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json.
kubectl get node node01 -o json > /opt/outputs/node01.json

# Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

# Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

# A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
kubectl config view --kubeconfig=/root/my-kube-config
kubectl config view --kubeconfig=/root/my-kube-config -o json
kubectl config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.users[*].name}' > /opt/outputs/users.txt
