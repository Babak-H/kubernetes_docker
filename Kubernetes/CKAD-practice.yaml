
- kube-apiserver---authorization-mode=Node,RBAC---advertise-address=172.17.0.18---allow-privileged=true---client-ca-file=/etc/kubernetes/pki/ca.crt---disable-admission-plugins=PersistentVolumeLabel---enable-admission-plugins=NodeRestriction---enable-bootstrap-token-auth=true---etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt---etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt---etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key---etcd-servers=https://127.0.0.1:2379---insecure-port=0---kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt---kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key---kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname---proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt---proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key---requestheader-allowed-names=front-proxy-client---requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt---requestheader-extra-headers-prefix=X-Remote-Extra----requestheader-group-headers=X-Remote-Group---requestheader-username-headers=X-Remote-User---secure-port=6443---service-account-key-file=/etc/kubernetes/pki/sa.pub---service-cluster-ip-range=10.96.0.0/12---tls-cert-file=/etc/kubernetes/pki/apiserver.crt---tls-private-key-file=/etc/kubernetes/pki/apiserve


openssl genrsa-out old-ca.key 2048 openssl req -new -key old-ca.key-subj "/CN=old-ca" -out old-ca.csr openssl x509 -req -in old-ca.csr-signkey old-ca.key-out old-ca.crt -days 365 openssl x509 -req -in ca.csr-signkey ca.key-out server.crt -days 365 openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl req -new -key apiserver-kubelet-client.key-out apiserver-kubelet-client.csr-subj "/CN=kube-apiserver-kubelet-client/O=system:masters" openssl x509 -req -in apiserver-kubelet-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-kubelet-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA /root/new-ca/old-ca.crt -CAkey /root/new-ca/old-ca.key-CAcreateserial-out apiserver-etcd-client-new.crt -days 365 openssl req -new -key /etc/kubernetes/pki/apiserver-etcd-client.key-out apiserver-etcd-client.csr-subj "/CN=kube-apiserver-etcd-client/O=system:masters" openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -days -10 openssl x509 -req -in apiserver-etcd-client.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver-etcd-client.crt -startdate 190101010101Z 20170101000000Z 200801010000Z "openssl", "req", "-new", "-key" ,"/etc/kubernetes/pki/apiserver-etcd-client.key", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-subj", "/CN=kube-apiserver-etcd client/O=system:masters" "openssl", "x509", "-req", "-in", "/etc/kubernetes/pki/apiserver-etcd-client.csr", "-CA", "/etc/kubernetes/pki/etcd/ca.crt", "-CAkey", "/etc/kubernetes/pki/etcd/ca.key", "-CAcreateserial", "-out", "/etc/kubernetes/pki/apiserver-etcd-client.crt" openssl x509 -req -in /etc/kubernetes/pki/apiserver-etcd-client.csr-CA /etc/kubernetes/pki/etcd/ca.crt -CAkey /etc/kubernetes/pki/etcd/ca.key-CAcreateserial-out /etc/kubernetes/pki/apiserver-etcd-client.crt -days 100 openssl x509 -req -in apiserver.csr-CA ca.crt -CAkey ca.key-CAcreateserial-out apiserver.crt

#=============================================================================================
# k run nginx-resources --image=nginx --restart=Never --dry-run=client -o yaml > pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-resources
  name: nginx-resources
spec:
  # restart policy is for the whole pod, not just a container
  restartPolicy: Never
  containers:
    - name: nginx-resources
      image: nginx
      resources:
        requests:
          cpu: 200m
          memory: 1Gi
#=============================================================================================
# The pod for the Deployment named 'nosql' in the 'craytisn' namespace fails to start because its container runs out of resources.
# Update the nosql Deployment so that the Pod:
# 1) Request 160M of memory for its Container
# 2) Limits the memory to half the maximum memory constraint set for the crayfish namespace

# k describe ns crayfish # we can see the resourceQuota and LimitRange in here for the namespace
# vi nosql.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nosql
  namespace: crayfish
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nosql
  template:
    metadata:
      labels:
        name: nosql
    spec:
      containers:
        - image: mongo:4.2
          name: mongo
          ports:
            - containerPort: 2701
          resources:
            requests:
              memory: 160Mi
            limits:
              memory: 320Mi
#===================================================================================
# k create cm another-config --from-literal=key4=value3
# k run nginx-configmap --image=nginx --dry-run=client -o yaml > pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-configmap
  name: nginx-configmap
spec:
  volumes:
    - name: config-vol
      configMap:
        name: another-config
  containers:
    - image: nginx
      name: nginx-configmap
      volumeMounts:
        - name: config-vol
          mountPath: /also/a/path
          
# k describe cm another-config
# k get pod nginx-configmap -o yaml

# configmap from file
# k -n moon create cm configmap-web-moon-html --from-file=index.html=/opt/course/15/web-moon.html
#=============================================================================================
# a deployment in 'production' namespace needs to run as specific serviceAccount

# k get sa -n production
# k -n production set serviceaccount deploy app-a restrictedservice 
# or
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-a
spec:
  replicas: 2
  selector:
    matchLabels:
      name: app-a
  template:
    metadata:
      labels:
        name: app-a
    spec:
      # add the custom serviceAccount in spec.template.spec.serviceAccountName, its inside the pod template
      serviceAccountName: restrictedservice
      containers:
        - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
          name: app-a
#=============================================================================================
# k create deploy backend-deployment --image=nginx --replicas=4 --port 8081 --dry-run=client -o yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: backend-deployment
  name: backend-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: backend-deployment
  template:
    metadata:
      labels:
        app: backend-deployment
    spec:
      containers:
        - image: nginx
          name: nginx
          ports:
            - containerPort: 8081
          # Probes are for each container in a pod/deployment
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 8
            periodSeconds: 5
#=============================================================================================
# k run cache --image=Ifccncf/redis:3.2 --port=6379 -n web
#=============================================================================================
# k create secret generic another-secret --from-literal=key1=value4
# k run nginx-secret --image=nginx --dry-run=client -o yaml > pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx-secret
  name: nginx-secret
spec:
  containers:
    - image: nginx
      name: nginx-secret
      env:
        - name: COOL_VARIABLE
          valueFrom:
            secretKeyRef:
              name: another-secret
              key: key1

# how to save a secret as volume
---
volumes:
  - name: secret2-volume
    secret:
      secretName: secret2
      
# get all the variables from the secret
---
envFrom:
  - secretRef:
      name: secret1
#=============================================================================================
# pod already exists, The application has an endpoint, /started, that will indicate if it can accept traffic by returning an HTTP 200. If the endpoint returns an HTTP 500, the application has not yet finished initialization => ReadinessProbe
# application has another endpoint /healthz that will indicate if the application is still working as expected by returning an HTTP 200. If the endpoint returns an HTTP 500 the application is no longer responsive => LivenessProbe
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod
  name: pod
spec:
  containers:
    - args:
        - probe-pod
      image: nginx
      name: pod
      readinessProbe:
        httpGet:
          path: /started
          port: 8080
      livenessProbe:
        httpGet:
          path: /healthz
          port: 8080
#=============================================================================================
# k run pod1 --image=Ifccncf/arg-output --dry-run=client -o yaml -- -line 56 -F > /opt/KDPD00101/pod1.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod1
  name: pod1
spec:
  containers:
    - name: app1cont  # change the pod name here
      image: Ifccncf/arg-output
      command:
        - -lines
        - '56'
        - -F
# k get po
# k get po pod1 -o json > /opt/KDPD00101/out1.json  # save the pod config as json file
#=============================================================================================
# k create deploy -n KDPD00101 frontend --replicas=4 --image=lfccncf/nginx:1.13.7 --port=8080 --dry-run=client -o yaml > pod.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: frontend
  name: frontend
  namespace: KDPD00101
spec:
  replicas: 4
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - image: lfccncf/nginx:1.13.7
          name: nginx
          ports:
            - containerPort: 8080
          env:
            - name: NGNIX_PORT
              value: "8080"  # env values should be string
#=============================================================================================
# Update the app deployment in the kdpd00202 namespace with a maxSurge of 5% and a maxUnavailable of 10%
# Perform a rolling update of the web1 deployment, changing the Ifccncf/ngmx image version to 1.13
# Roll back the app deployment to the previous version

# k edit deploy nginx-deployment -n kdpd00202
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: kdpd00202
  labels:
    app: nginx
spec:
  replicas: 3
  # strategy
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          # to execute the rolling update, we can just edit the image inside the deployment yaml and it will update the pods one by one
          image: Ifccncf/ngmx:1.13
          ports:
            - containerPort: 80
  # rolling update is OUTSIDE the pod and under deployment spec
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 5%
      maxUnavailable: 10%
      
# k rollout status deploy nginx-deployment -n kdpd00202  # wait for all pods to be updated
# k rollout undo deploy nginx-deployment -n kdpd00202 # rollback to previous revision
# k rollout history deploy nginx-deployment -n kdpd00202

# when you have a list of rollouts and current one isn't working, only way to go back to a working one is to go back
# one by one to see which one worked before
# k rollout history deploy my-nginx
# k rollout undo deploy my-nginx --to-revision=4
#=============================================================================================
# add tag 'func: webFrontEnd' to already existing deployment in pod template and make sure it has 5 replicas

# k edit deploy kdsn00101-deployment -n kdsn00101
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kdsn00101-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
        func: webFrontEnd
    spec:
      containers:
        - name: kdsn00101
          image: fhsinchy/notes-api
          ports:
            - containerPort: 8080
            
# create a nodePort service for the deployment and use newly added tag
# k expose deploy kdsn00101-deployment -n kdsn00101 --type NodePort --port 8080 --targe-port 8080 --name cherry
# or
---
apiVersion: v1
kind: Service
metadata:
  name: cherry
spec:
  type: NodePort
  selector:
    func: webFrontEnd
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
#     nodePort: it is defined automatically if not defined

# how to test a working service from a temporary pod
# find service endpoint
# k -n pluto describe svc project-plt-svc | grep -i endpoints
# k run temp --image=nginx:alpine -- curl http://project-plt-svc:3333

# how to test curl on a pod without svc
# k get po -o wide # get pod's ip address
# k run temp --image=nginx:alpine -- curl 10.44.0.78
# k logs temp
#=============================================================================================
# Configuring security contexts for Pods and containers.

# Modify the existing Deployment named broker-deployment running in namespace quetzal so that its containers:
# 1) Run with user ID 30000 and
# 2) Privilege escalation is forbidden
# The broker-deployment is manifest file can be found at: my-deploy.yml

# vi my-deploy.yaml
# k apply -f my-deploy.yaml
# k get pods -n quetzal
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broker-deployment
  namespace: quetzal
  labels:
    app: broker-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: broker-deployment
  template:
    metadata:
      name: myapp-pod
      labels:
        app: broker-deployment
    spec:
      containers:
        - name: broker
          image: redis:alpine
          ports:
            - containerPort: 6379
          securityContext:
            runAsUser: 30000
            allowPrivilegeEscalation: false
#=============================================================================================
# NetworkPolicy to allow specific traffic.
# Update the Pod ckad00018-newpod in the ckad00018 namespace to use a NetworkPolicy allowing the Pod to send and receive traffic only to and from the pods web and db => give it additional labels
# all required network policies have already been created, do not create or edit them!

# k get netpol -n ckad00018
# k describe netpol my-current-netpol-1 -n ckad00018  # check what tags the pod needs to send/recieve traffic from "web" and "db" pods

# k label pod ckad00018-newpod -n ckad00018 web-access=true
# k label pod ckad00018-newpod -n ckad00018 db-access=true

# pod front-end has label id: frontend
# pod api has label id: api
# create a network policy that pod frontend can only send the data to pod api
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np1
  namespace: venus
spec:
  podSelector:
    matchLabels:
      id: frontend # label of the pods this policy should be applied on
  policyTypes:
  - Egress # we only want to control egress
  egress:
    - to:
        - podSelector: # allow egress only to pods with api label
            matchLabels:
              ip: api
    # this is under egress, but NOT related to porSelector rules, # these ports are allowed on all outgoing pods
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
#=============================================================================================
# multi-container pod
# we have a container that writes log files in format A and another that converts logs to format B, create a deployment
# that runs both containers
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-container-app
spec:
  replicas: 2
  selector:
    matchLabels:
      run: app
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
        - name: web-one
          image: busybox:1.28
          command: ["/bin/sh", "-c", "while true; do echo 'i love cnfc' > /tmp/log/input.log; sleep 10; done"]
          volumeMounts:
            - name: log-volume
              mountPath: /tmp/log
        - name: side-car
          image: busybox:1.28
          volumeMounts:
            - name: log-volume
              mountPath: /tmp/log
      volumes:
        - name: log-volume
          emptyDir: {}
          
# Create a sidecar container named logger-con, image busybox:1.31.0 , which mounts the same volume and writes the content of cleaner.log to stdout, you can use the tail -f command
# Sidecar containers in K8s are initContainers with restartPolicy: Always
---
initContainers:
  - name: logger-con
    image: busybox:1.31.0
    restartPolicy: Always  # sidecar containers should have always restarted policy, because we want to continue running them
    command: ["/bin/sh", "-c", "tail -f /var/log/cleaner/cleaner.log"]
    volumeMounts:
        - name: logs
          mountPath: /var/log/cleaner

# you can't mount a volume to root of a container!  mountPath: /  => will fail

#=============================================================================================
# PVC, PVC, StorageClass

#  PV doesn't need a namespace
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: earth-project-earthflower-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /Volumes/Data

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: earth-project-earthflower-pvc
  namespace: earth
spec:
  resources:
    requests:
      storage: 2Gi
  accessModes:
    - ReadWriteOnce

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: project-earthflower
  namespace: earth
spec:
  replicas: 1
  selector:
    matchLabels:
      app: project-earthflower
  template:
    metadata:
      labels:
        app: project-earthflower
    spec:
      volumes:
        - name: my-vol
          persistentVolumeClaim:
            claimName: earth-project-earthflower-pvc
      containers:
        - name: container
          image: httpd:2.4.41-alpine
          volumeMounts:
            - name: my-vol
              mountPath: /tmp/project-data

# to get pvc events
# k -n earth describe pvc earth-project-earthflower-pvc # Event section is at end
# manually copy it to file /opt/course/13/pvc-126-reason

# This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain
#  storageClass doesn't need a namespace
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: moon-retain
provisioner: moon-retainer
reclaimPolicy: Retain
#==============================================

k run webapp-green --image=kodekloud/webapp-color -- --color green
---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
    ################################3
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
---

kubectl create token dashboard-sa

kubectl set serviceaccount deploy/web-dashboard dashboard-sa

---

kubectl create secret docker-registry private-reg-cred --docker-server=myprivateregistery.com:5000 \
--docker-username=dock_user --docker-password=dock_password --docker-email=docker_user@myprivateregistery.com

---
apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  containers:
  - name: nginx
    image: myprivateregistery.com:5000/nginx:alpine
  imagePullSecrets:
  - name: private-reg-cred
---

cat /usr/include/linux/capability.h

---
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu    # runs as user 1002
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu  # runs as user 1001
     name: sidecar
     command: ["sleep", "5000"]
---
# A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt
kubectl get pv -o json
kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt
#============================
# https://github.com/bmuschko/cka-crash-course

# untaint node
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

# The application stores logs at location /log/app.log. View the logs.
# You can exec in to the container and open the file:
kubectl exec webapp -- cat /log/app.log


# What network range are the nodes in the cluster part of?

# find the Internal IP of the nodes.
ip a | grep eth0
# Next, use the ipcalc tool to see the network details:
ipcalc -b 192.14.41.6/24 
        Address:   192.14.41.6          
        Netmask:   255.255.255.0 = 24   
        Wildcard:  0.0.0.255            
        =>
        Network:   192.14.41.0/24       
        HostMin:   192.14.41.1          
        HostMax:   192.14.41.254        
        Broadcast: 192.14.41.255        
        Hosts/Net: 254                   Class C

# Network:   192.14.41.0/24    is the ip range


# What is the range of IP addresses configured for PODs on this cluster?

# The network is configured with weave. Check the weave pods logs
controlplane ~ ➜  k logs weave-net-b9kh7 -n kube-system | grep ipalloc
Defaulted container "weave" out of: weave, weave-npc, weave-init (init)
INFO: 2024/11/26 14:03:24.230278 Command line options: map[conn-limit:200 datapath:datapath db-prefix:/weavedb/weave-net docker-api: expect-npc:true http-addr:127.0.0.1:6784 
ipalloc-init:consensus=1 ipalloc-range:10.244.0.0/16 metrics-addr:0.0.0.0:6782 name:b2:c2:ed:7f:e2:78 nickname:node01 no-dns:true no-masq-local:true port:6783]

# ipalloc-range:10.244.0.0/16

# What is the IP Range configured for the services within the cluster?

# Inspect the setting on kube-api server 
controlplane ~ ➜  cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range
    - --service-cluster-ip-range=10.96.0.0/12

# What type of proxy is the kube-proxy configured to use?

# Check the logs of the kube-proxy pods
controlplane ~ ➜  k logs kube-proxy-6xjw6 -n kube-system
I1126 14:02:33.141546       1 server_linux.go:66] "Using iptables proxy"
I1126 14:02:33.428064       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.14.41.6"]


# Identify the DNS solution implemented in this cluster.
controlplane ~ ➜  kubectl get pods -n kube-system | grep dns
coredns-77d6fd4654-5h89h               1/1     Running   0          2m27s
coredns-77d6fd4654-mczjv               1/1     Running   0          2m27s

# What is the IP of the CoreDNS server that should be configured on PODs to resolve services?
controlplane ~ ➜  kubectl get svc -n kube-system | grep dns
kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP,9153/TCP   3m13s

# Where is the configuration file located for configuring the CoreDNS service?

# Inspect the Args field of the coredns deployment and check the file used.
controlplane ~ ➜  kubectl -n kube-system describe deployment coredns | grep -A2 Args
    Args:
      -conf
      /etc/coredns/Corefile

# How is the Corefile passed into the CoreDNS POD?
controlplane ~ ➜  kubectl get cm -n kube-system | grep dns
coredns                                                1      7m3s

# What is the root domain/zone configured for this kubernetes cluster?
k get cm coredns -n kube-system -o yaml

        apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }

# cluster.local

# Which of the below name CANNOT be used to access the payroll service from the test application?

        controlplane ~ ➜  k get po 
        NAME                READY   STATUS    RESTARTS   AGE
        hr                  1/1     Running   0          8m31s
        simple-webapp-1     1/1     Running   0          8m12s
        simple-webapp-122   1/1     Running   0          8m12s
        test                1/1     Running   0          8m31s

        controlplane ~ ➜  k get svc
        NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
        kubernetes     ClusterIP   172.20.0.1       <none>        443/TCP        9m26s
        test-service   NodePort    172.20.90.60     <none>        80:30080/TCP   8m33s
        web-service    ClusterIP   172.20.203.129   <none>        80/TCP         8m34s

        controlplane ~ ➜  k get po -n payroll
        NAME   READY   STATUS    RESTARTS   AGE
        web    1/1     Running   0          8m40s

        controlplane ~ ➜  k get svc -n payroll
        NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
        web-service   ClusterIP   172.20.140.81   <none>        80/TCP    8m48s

# web-service.payroll.svc.cluster.local   can
# web-service.payroll.svc.cluster    CAN'T
# web-service.payroll.svc   can
# web-service.payroll   can


# From the hr pod nslookup the mysql service (in payroll namespace) and redirect the output to a file /root/CKA/nslookup.out
controlplane ~ ✖ kubectl exec -it hr -- nslookup mysql.payroll.svc.cluster.local > /root/CKA/nslookup.out

----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort

#-----------------------------------
# find all static pods
k get po -A | grep controlplane
k get po -A -o wide | grep controlplane

/etc/kubernetes/manifests

# how to find ip address of a node
k get nodes -o wide

# get all events
k get events -o wide

------------
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: registry.k8s.io/kube-scheduler:v1.31.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
------------------
apiVersion: v1
kind: Pod 
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  schedulerName: my-scheduler
----------------

k drain node01 --ignore-daemonsets

k uncordon node01

k cordon node01
---

# Upgrade MasterNode (controlplane)

cat /etc/*release*

kubectl get nodes

kubectl get nodes -o wide

kubectl describe nodes node01

kubeadm version

kubeadm upgrade plan

kubectl drain controlplane --ignore-daemonsets

vim /etc/apt/sources.list.d/kubernetes.list

  deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /

apt update

apt-cache madison kubeadm

apt-get install kubeadm=1.31.0-1.1

kubeadm upgrade plan v1.31.0

kubeadm upgrade apply v1.31.0

apt-get install kubelet=1.31.0-1.1

kubectl uncordon controlplane

kubectl get nodes -o wide

# Upgrade WorkerNode

kubeadm version

kubectl drain node01 --ignore-daemonsets

kubectl get pods -o wide

ssh node01

vi /etc/apt/sources.list.d/kubernetes.list

  deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /

apt update

apt-cache madison kubeadm

apt-get install kubeadm=1.31.0-1.1

# Upgrade the node 
kubeadm upgrade node

apt-get install kubelet=1.31.0-1.1

systemctl daemon-reload

systemctl restart kubelet

exit

kubectl get nodes -o wide

kubectl uncordon node01
---

kubectl logs etcd-controlplane -n kube-system | grep -i 'etcd-version'

kubectl describe po etcd-controlplane -n kube-system

kubectl describe pod etcd-controlplane -n kube-system | grep -i '\--listen-client-urls'

kubectl describe pod etcd-controlplane -n kube-system | grep -i '\--cert-file'

kubectl describe pod etcd-controlplane -n kube-system | grep -i '\--peer-cert-file='

kubectl describe pod etcd-controlplane -n kube-system | grep -i '\--cert-file='

kubectl describe pod etcd-controlplane -n kube-system | grep -i '\--trusted-ca'

# create a ETCD backup

# --endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
# --cacert: Mandatory Flag (Absolute Path to the CA certificate file)
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
# --cert: Mandatory Flag (Absolute Path to the Server certificate file)
--cert=/etc/kubernetes/pki/etcd/server.crt \
# --key: Mandatory Flag (Absolute Path to the Key file)
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

ls /opt

# restore ETCD backup

ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db

vi /etc/kubernetes/manifests/etcd.yaml

    volumes:
    - hostPath:
        path: /var/lib/etcd-from-backup
        type: DirectoryOrCreate
      name: etcd-data

    volumeMounts:
    - mountPath: /var/lib/etcd-from-backup
      name: etcd-data

watch "crictl ps | grep etcd"

k get po -n kube-system

k get deploy -n web-apps


# backup ETCD from another machine/cluster

k get nodes
ssh cluster1-controlplane
cat /etc/kubernetes/manifests/etcd.yaml 

kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls

kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep pki

ETCDCTL_API=3 etcdctl --endpoints=https://192.160.244.10:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster1.db
# copy the backup to another machine
scp cluster1-controlplane:/opt/cluster1.db /opt


# restore and backup ETCD from another machine/cluster with External ETCD server

kubectl config get-contexts
kubectl config get-clusters

kubectl config use-context cluster1
kubectl config use-context cluster2

k get po -n kube-system  | grep etcd

ls /etc/kubernetes/manifests/ | grep -i etcd

ps ef | grep -i etcd

kubectl -n kube-system describe pod kube-apiserver-cluster2-controlplane

# external etcd (ETCD server not running on this cluster)
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem
      --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem
      --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem
      --etcd-servers=https://192.22.46.12:2379

ssh cluster2-controlplane ps -ef | grep etcd
ssh etcd-server
ps -ef | grep --color=auto etcd


Check the members of the ETCD cluster:
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
   member list

59ee55985632d394, started, etcd-server, https://192.160.244.3:2380, https://192.160.244.3:2379, false  => only one node in ETCD server

# back up is created in another machine, copy it to the etcd server
scp /opt/cluster2.db etcd-server:/root

ETCDCTL_API=3 etcdctl snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new

ls -la /var/lib
# it should be owned by etcd user/group not root
chown -R etcd:etcd etcd-data-new/

# edit the etcd service so it uses new data
vi /etc/systemd/system/etcd.service

    --data-dir=/var/lib/etcd-data-new

# reload the daemon and then etcd service
systemctl daemon-reload
systemctl restart etcd

systemctl status etcd

# go back to controlplane node and restart kube-controller-manager, kube-scheduler pods and kublet service to make sure they use new etcd
kubectl delete pods kube-controller-manager-* kube-scheduler-* -n kube-system

systemctl restart kubelet

kubectl get deployment -n web-apps

---
# Identify the "certificate" file used for the kube-api server
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep tls-cert-file
  --tls-cert-file=/etc/kubernetes/pki/apiserver.crt

# Identify the "Certificate" file used to authenticate kube-apiserver as a client to ETCD Server
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd-certfile
        - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt

# Identify the "key" used to authenticate kubeapi-server to the kubelet server
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep kubelet-client-key
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key

# Identify the ETCD Server Certificate used to host ETCD server.
cat /etc/kubernetes/manifests/etcd.yaml | grep cert-file
- --cert-file=/etc/kubernetes/pki/etcd/server.crt

# Identify the ETCD Server CA Root Certificate used to serve ETCD Server
# ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server.
 cat /etc/kubernetes/manifests/etcd.yaml | grep trusted-ca-file
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt

# What is the Common Name (CN) configured on the Kube API Server Certificate?
cat /etc/kubernetes/pki/apiserver.crt
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

What is the name of the CA who issued the Kube API Server Certificate?
  kubernetes

                  Certificate:
                      Data:
                          Version: 3 (0x2)
                          Serial Number: 3378315762146779274 (0x2ee230495ddca08a)
                          Signature Algorithm: sha256WithRSAEncryption
                          Issuer: CN = kubernetes
                          Validity
                              Not Before: Nov 16 14:16:29 2024 GMT
                              Not After : Nov 16 14:21:29 2025 GMT
                          Subject: CN = kube-apiserver
                          Subject Public Key Info:
                              Public Key Algorithm: rsaEncryption
                                  Public-Key: (2048 bit)
                                  Modulus:
                                      00:be:35:d0:06:3a:b0:f5:11:c9:15:96:23:50:8b:
                                      52:d3:aa:9e:b4:e6:5f:ea:4d:4f:50:8a:d1:b4:a3:
                                      91:1f:0a:dd:96:19:dc:5a:74:fd:db:12:c9:7b:3f:
                                      0a:5f:14:24:b3:ed:a7:f5:ad:14:bc:27:1a:6b:e3:
                                      b1:e1:a6:0f:15:a2:52:72:42:5d:1a:c6:40:a5:18:
                                      bf:af:b4:e5:59:f4:a2:06:1c:20:84:c8:22:c1:46:
                                      5f:6e:02:2d:4f:f9:38:e5:9b:a8:db:40:bb:5f:15:
                                      20:a1:fe:b6:60:5d:cf:e4:6e:71:05:d3:e7:bb:b1:
                                      7a:05:2b:ea:37:11:2b:2f:ed:2c:e3:4e:a2:6e:6c:
                                      de:3e:0d:7e:71:0d:cf:08:51:bc:a3:01:0e:81:b8:
                                      d9:f3:26:e7:a5:4b:04:e6:13:13:bb:07:3e:6f:be:
                                      da:9e:26:0d:1d:a6:fc:37:d3:de:f0:a3:a5:e3:c2:
                                      69:f8:e2:a2:85:50:d7:57:df:0c:96:25:ae:f5:f0:
                                      ba:5d:c6:f1:9a:a8:d5:0c:12:99:5a:af:f8:8b:2e:
                                      bc:6b:f7:09:d0:a6:74:82:74:a2:e6:12:bc:8a:76:
                                      2c:7d:26:a4:21:10:7e:b0:44:e5:9e:08:3c:55:04:
                                      fe:16:8b:e3:27:0e:7b:78:da:57:19:97:9e:da:d8:
                                      bc:17
                                  Exponent: 65537 (0x10001)
                          X509v3 extensions:
                              X509v3 Key Usage: critical
                                  Digital Signature, Key Encipherment
                              X509v3 Extended Key Usage:
                                  TLS Web Server Authentication
                              X509v3 Basic Constraints: critical
                                  CA:FALSE
                              X509v3 Authority Key Identifier:
                                  A1:0C:59:02:F4:28:DA:9E:CC:F7:77:A2:30:EF:68:D5:47:1D:8C:0E
                              X509v3 Subject Alternative Name:
                                  DNS:controlplane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:192.15.155.9
                      Signature Algorithm: sha256WithRSAEncryption
                      Signature Value:
                          65:44:6e:38:d5:9e:bf:97:c0:c7:58:56:ea:7f:c5:6f:6b:71:
                          d4:1d:6b:e5:5a:a5:94:25:3a:36:41:dd:c5:25:39:01:95:43:
                          77:41:56:ea:57:ad:53:65:fe:18:d2:fa:ee:50:3b:b4:44:f9:
                          b1:b3:65:a3:08:a7:b9:cc:24:21:5e:95:81:e1:2a:53:d8:24:
                          b2:64:76:80:60:51:3f:cd:1b:12:0e:60:6d:c9:79:69:bc:bc:
                          e6:f8:ad:1c:12:f2:20:08:71:9e:28:26:e8:0c:8e:d3:03:7a:
                          53:d1:47:52:fa:ea:34:d3:2c:3c:3a:a2:cb:18:b5:a9:28:99:
                          e7:f5:b6:be:ee:1d:c2:57:a5:40:24:7f:dd:65:0a:c1:46:ba:
                          79:5f:b7:ce:ae:03:6c:4f:ca:9d:c5:e8:85:af:cb:85:81:db:
                          65:a5:69:1f:27:46:6f:4e:f5:2f:7f:ec:2f:42:b4:62:d8:65:
                          0c:b2:12:6b:dc:e9:78:59:36:29:96:e3:dc:1d:e3:d0:1e:71:
                          bc:89:29:b8:4e:f3:c5:85:a4:e5:f7:65:98:1f:cc:1c:da:44:
                          1a:f9:d1:73:f6:8f:de:1e:09:15:e7:71:df:9d:ff:2f:01:36:
                          c4:28:7d:b1:6b:c3:8f:b2:7d:70:52:31:ee:68:6f:0f:d4:41:
                          1a:3e:13:68


# What is the Common Name (CN) configured on the ETCD Server certificate?
cat /etc/kubernetes/pki/etcd/server.crt
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout
     CN = etcd-ca

# How long, from the issued date, is the Kube-API Server Certificate valid for?
           Validity
                Not Before: Nov 16 14:16:29 2024 GMT
                Not After : Nov 16 14:21:29 2025 GMT

# How long, from the issued date, is the Root CA Certificate valid for?
--client-ca-file=/etc/kubernetes/pki/ca.crt
ls /etc/kubernetes/pki/
openssl x509 -in /etc/kubernetes/pki/ca.crt -text --noout

              Validity
                  Not Before: Nov 16 14:16:29 2024 GMT
                  Not After : Nov 14 14:21:29 2034 GMT


# Kubectl suddenly stops responding to your commands. Check it out! Someone recently modified the /etc/kubernetes/manifests/etcd.yaml file
ls -l /etc/kubernetes/pki/etcd/server* | grep .crt
  -rw-r--r-- 1 root root 1208 Nov 16 14:21 /etc/kubernetes/pki/etcd/server.crt
# Update the YAML file with the correct certificate path and wait for the ETCD pod to be recreated. wait for the kube-apiserver to get to a Ready state.


#The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue.
#Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs
docker ps -a | grep kube-apiserver
docker logs --tail=2 1fb242055cff8
    W0916 14:19:44.771920       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: authentication handshake failed: x509: certificate signed by unknown authority". Reconnecting...
    E0916 14:19:48.689303       1 run.go:74] "command failed" err="context deadline exceeded"

docker ps -a | grep etcd
docker logs 1f24332055cfvnv    # you can see connection errors here

# "127.0.0.1:2379" is address of ETCD server
# This indicates an issue with the ETCD CA certificate used by the kube-apiserver. Correct it to use the file /etc/kubernetes/pki/etcd/ca.crt
vi /etc/kubernetes/manifests/kube-apiserver.yaml
          - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt

--------

# Create a CertificateSigningRequest object with the name akshay with the contents of the akshay.csr file

cat akshay.csr | base64 -w 0

---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
    - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXpRL1dXSGFrU25ycFA4ZVBEWTlITTFnT3M4UWpFd3FoQ0JrRlVsQ2dodnNXCktUMHhpZWJpUk9pSjQyb2lrVjVodHprZE5tZHRpczdEWmk0ck5aRTh3cWZNRUF4NUpJRjVwTWQ3Z2t2VGVyZWsKRW5SVFYyWGNnWlExS3VsZUNheEh3QWpUVnExdXNPMXVvR0E1U1pYWm5NWVZJSXFRV2g4M1dhZFoyRmhFMk5yKwpxU3g3cHV1SEg0S0RUQnZBMlJsY3NyQ1BqZ3FYcEZINStVenl5dm1zT2RJZ3hEcWZsNUxOcDQzM0lHQkRRQk41ClBvVGRLQVdiOXo2SWtlVSs4ZUs3czEydmk0Vk1BZHNZcERzaG1vVEJBV0FkNU5xbTlvRUM5VkRGcTgxSVgvUzAKdUc3TVRLOWRoU2MrcDBGUDFKaEZtWk9kd3c5d1JwdituRm5KSjZjVWRRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBRmdzaU5lQXBsNWVMVmNyWDFSOStMSzlKai9PeS9lN24rQ2paR1lUcVB4Si9ZUCtCcDVKCkdGMkxPNVdoSDdDNVBWU0tXMjRWbVpXR09zUERZbGswUHVxL0tXbWlKT2pQTzNERk96bURMYjV2eEtUaFJHYm8KMy9BL3B6VEtDNlpHNTZqL21hY2xVejVIcEZSazVLYnQxeTF3RzVJUjUxclMzNFF1TEpyKzZNU2ZZaGkwWVVxagpOYllKOU5WSDQ5WUVTSUFManFoZWZydjV2SmhDR2ppMlJwblZIL0xkYnNLdUJ4SllyZTAxendMYlcrWE5yaWFTCkRkRE13OUdNcmxITENqVkRsbFE0SEhTUUxISlZ0aWJZN3RFbWxBb0tzdTZISkR1S0hjVnVLeXAwZVJ3VjBLUGsKZkkzWEZxbnJOUUhIMUlJZytFVXd6RmNKQVFQY2lWUXFOT0U9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  # Please note that an additional field called signerName should also be added when creating CSR.
  # For client authentication to the API server we will use the built-in signer kubernetes.io/kube-apiserver-client
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth


kubectl apply -f akshay-csr.yaml

kubectl get csr
kubectl get csr akshay
    NAME     AGE   SIGNERNAME                            REQUESTOR          REQUESTEDDURATION   CONDITION
    akshay   44s   kubernetes.io/kube-apiserver-client   kubernetes-admin   <none>              Pending



kubectl certificate approve akshay
    certificatesigningrequest.certificates.k8s.io/akshay approved

kubectl get csr agent-smith -o yaml

kubectl certificate deny agent-smith
    certificatesigningrequest.certificates.k8s.io/agent-smith denied

kubectl delete csr agent-smith

---

kubectl config --kubeconfig=/root/my-kube-config use-context research

kubectl config --kubeconfig=/root/my-kube-config current-context

# change the current kubeconfig file
cp my-kube-config ~/.kube/config

# The path to certificate is incorrect in the kubeconfig file. Correct the certificate name which is available at /etc/kubernetes/pki/users/.

----

# Inspect the environment and identify the authorization modes configured on the cluster.
kubectl describe pod kube-apiserver-controlplane -n kube-system | grep auth
    --authorization-mode=Node,RBAC

# A user dev-user is created. User's details have been added to the kubeconfig file. Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
cat /root/.kube/config

kubectl get pods --as dev-user
    Error from server (Forbidden): pods is forbidden: User "dev-user" cannot list resource "pods" in API group "" in the namespace "default"

# if you want to give access to user to see objects in different namespace, then he needs role and rolebinding that exist in that namespace

#============================

# What is the network interface configured for cluster connectivity on the controlplane node?

# kubectl get nodes -o wide to see the IP address assigned to the controlplane node
kubectl get nodes controlplane -o wide

# TO DO THIS YOU NEED TO SSH INSIDE THE NODE
# Next, find the network interface to which this IP is assigned by making use of the "ip link" command
ip a | grep 192.168.227.81
    inet 192.168.227.81/32 scope global eth0

# Here you can see that the interface associated with this IP is eth0 on the host.

---

# What is the MAC address of the interface on the controlplane node?

# use the network interface that was discoved before to find the mac address
ip link show eth0
      3: eth0@if4065: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1410 qdisc noqueue state UP mode DEFAULT group default
          link/ether 26:87:84:a7:3e:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0

---

# We use "Containerd" as our container runtime. What is the interface/bridge created by Containerd on the controlplane node?

# Run the command: ip link and look for a bridge interface created by containerd.
ip address show type bridge
    cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1360 qdisc noqueue state UP group default qlen 1000
        link/ether e6:51:25:e1:b9:6a brd ff:ff:ff:ff:ff:ff
        inet 172.17.0.1/24 brd 172.17.0.255 scope global cni0
           valid_lft forever preferred_lft forever
        inet6 fe80::e451:25ff:fee1:b96a/64 scope link
           valid_lft forever preferred_lft forever

---

# What is the state of the interface cni0?
ip link show cni0
    5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1360 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether e6:51:25:e1:b9:6a brd ff:ff:ff:ff:ff:ff

---

# If you were to ping google from the controlplane node, which route does it take?

# What is the IP address of the Default Gateway?
ip route show default
  default via 169.254.1.1 dev eth0

----
# What is the port the kube-scheduler is listening on in the controlplane node?

#  using the netstat command and searching for the scheduler process
netstat -nplt | grep scheduler
    tcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      3665/kube-scheduler

# kube-scheduler process binds to the port 10259 on the controlplane node

---

# ETCD is listening on two ports. Which of these have more client connections established?
netstat -nplt | grep etcd
    tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3182/etcd
    tcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      3182/etcd
    tcp        0      0 192.168.231.169:2380    0.0.0.0:*               LISTEN      3182/etcd
    tcp        0      0 192.168.231.169:2379    0.0.0.0:*               LISTEN      3182/etcd

controlplane ~ ➜  netstat -anp | grep etcd | grep 2380 | wc -l
1

controlplane ~ ➜  netstat -anp | grep etcd | grep 2379 | wc -l
61

controlplane ~ ➜  netstat -anp | grep etcd | grep 2381 | wc -l
1

# That's because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd peer-to-peer connectivity. When you have multiple controlplane nodes. In this case we don't.
---

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

---

# Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes.

ps -aux | grep kubelet | grep container-runtime-endpoint
    root        4086  0.0  0.1 3005224 94652 ?       Ssl  14:15   0:16 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

# What is the path configured with all binaries of CNI supported plugins?

#all plugins are stored at /opt/cni/bin

# what is the CNI plugin configured to be used on this kubernetes cluster?
ls /etc/cni/net.d/
    10-flannel.conflist

# what binary executable file will be run by kubelet after a container and its associated namespace are created?
cat /etc/cni/net.d/10-flannel.conflist | grep type
  "flannel"

---

# deploy weave-net networking solution to the cluster:

# download the weavenet yaml file
wget https://github.com/weavework/*****

# check kubeproxy
kubectl describe pod kube-proxy-XXX -n kube-system | grep --config
    --config=/var/lib/kube-proxy/config.conf

kubectl describe configmap kube-proxy -n kube-sytem | grep clusterCIDR
    clusterCIDR: 10.244.0.0/16

vi weave-daemonset-k8s.yaml
# go to containers section, find weave container and set the environment variable accodring to clusterCIDR:
    env:
      - name: IPALLOC_RANGE
        value: 10.244.0.0/16

kubectl apply -f weave-daemonset-k8s.yaml

# should be one weave daemonset on each node
kubectl get pods -n kube-system | grep weave

---
# how many weave agents are deployed on this cluster?
kubectl get pod -n kube-system | grep weave

# on which nodes are the weave peers present?
kubectl get pod -n kube-system -o wide | grep weave

# identify name of the bridge network/interface created by weave on each node
ip add | grep weave

# what is the pod ip adress range configured by weave?
kubectl logs weave-net-XXX -n kube-system | grep ipalloc-range
    ipalloc-range: 10.244.0.0/16


# what is the default gateway configured on pods scheduled on "node01" ?
# try scheduling a pod on node01 and check the "ip route" output
kubectl run busybox --image=busybox --dry-run=client -o yaml -- sleep 1000 > busybox.yaml

vi busybox.yaml
  spec:
    nodeName: node01

kubectl apply -f busybox.yaml

kubectl exec busybox -- ip route
    default via10.244.192.0 dev eth0

---
# Install the kubeadm and kubelet packages on the controlplane and node01 nodes. Use the exact version of 1.31.0-1.1 for both.
# These steps have to be performed on both nodes:

*** install container runtime (containerd) ***
# Enable IPv4 packet forwarding

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
# Verify that net.ipv4.ip_forward is set to 1 with:
sysctl net.ipv4.ip_forward


# Update the apt package index and install packages needed to use the Kubernetes apt repository:
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl

# Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:
# # If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
## sudo mkdir -p -m 755 /etc/apt/keyring
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the appropriate Kubernetes apt repository. Please note that this repository have packages only for Kubernetes 1.31; for other Kubernetes minor versions,
# you need to change the Kubernetes minor version in the URL to match your desired minor version (you should also check that you are reading the documentation
# for the version of Kubernetes that you plan to install).
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:
sudo apt-get update
# To see the new version labels
sudo apt-cache madison kubeadm
sudo apt-get install -y kubelet=1.31.0-1.1 kubeadm=1.31.0-1.1 kubectl=1.31.0-1.1
sudo apt-mark hold kubelet kubeadm kubectl

# Enable the kubelet service before running kubeadm:
sudo systemctl enable --now kubelet

# What is the version of kubelet installed?
kubelet --version
    Kubernetes v1.31.0

# Initialize Control Plane Node (Master Node). Use the following options:
  # apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
  # apiserver-cert-extra-sans - Set it to controlplane
  # pod-network-cidr - Set to 10.244.0.0/16
  # Once done, set up the default kubeconfig file and wait for node to be part of the cluster.

# You can use the below kubeadm init command to spin up the cluster:

IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')

kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16

/*

[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0529 15:35:11.112522   11469 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.133.43.3]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 553.930452ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 12.503398796s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 90l0iw.8sa7trjypfybs5l1
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.133.43.3:6443 --token 90l0iw.8sa7trjypfybs5l1 \
        --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d

*/

# Once the command has been run successfully, set up the kubeconfig:
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config


# Generate a kubeadm join token Or copy the one that was generated by kubeadm init command

kubeadm join 192.133.43.3:6443 --token 90l0iw.8sa7trjypfybs5l1 \
        --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d

          [preflight] Running pre-flight checks
          [preflight] Reading configuration from the cluster...
          [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
          [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
          [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
          [kubelet-start] Starting the kubelet
          [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
          [kubelet-check] The kubelet is healthy after 1.00098712s
          [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

          This node has joined the cluster:
          * Certificate signing request was sent to apiserver and a response was received.
          * The Kubelet was informed of the new secure connection details.

          Run 'kubectl get nodes' on the control-plane to see this node join the cluster.


# To install a network plugin, we will go with Flannel as the default choice. For inter-host communication, we will utilize the eth0 interface.
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yaml

kubectl get pods -n kube-flannel

----

# The cluster is broken. We tried deploying an application but it's not working. Troubleshoot and fix the issue.

k get pods -n kube-system
      NAME                                   READY   STATUS             RESTARTS      AGE
      coredns-77d6fd4654-dgsgn               1/1     Running            0             17m
      coredns-77d6fd4654-tkz6s               1/1     Running            0             17m
      etcd-controlplane                      1/1     Running            0             17m
      kube-apiserver-controlplane            1/1     Running            0             17m
      kube-controller-manager-controlplane   1/1     Running            0             17m
      kube-proxy-vxjlk                       1/1     Running            0             17m
      kube-scheduler-controlplane            0/1     CrashLoopBackOff   8 (28s ago)   16m ####

kubectl get pod kube-scheduler-controlplane -n kube-system -o wide
    NAME                          READY   STATUS             RESTARTS        AGE   IP               NODE           NOMINATED NODE   READINESS GATES
    kube-scheduler-controlplane   0/1     CrashLoopBackOff   8 (2m47s ago)   18m   192.168.122.99   controlplane   <none>           <none>

# kube-scheduler is an static pod
cat /etc/kubernetes/manifests/kube-scheduler.yaml

vi /etc/kubernetes/manifests/kube-scheduler.yaml

      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          kubernetes.io/config.hash: 3aa60fbba62f9faa79076d6a3f6cb9ba
          kubernetes.io/config.mirror: 3aa60fbba62f9faa79076d6a3f6cb9ba
          kubernetes.io/config.seen: "2024-11-30T12:48:31.703014656Z"
          kubernetes.io/config.source: file
        creationTimestamp: "2024-11-30T12:48:42Z"
        labels:
          component: kube-scheduler
          tier: control-plane
        name: kube-scheduler-controlplane
        namespace: kube-system
        ownerReferences:
        - apiVersion: v1
          controller: true
          kind: Node
          name: controlplane
          uid: e1f395ba-f385-4403-890e-2280f0b8293e
        resourceVersion: "1371"
        uid: f8483153-c28f-464d-a6ba-1752d85d1d77
      spec:
        containers:
        - command:
          - kube-scheduler
          - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
          - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
          - --bind-address=127.0.0.1
          - --kubeconfig=/etc/kubernetes/scheduler.conf
          - --leader-elect=true
          image: registry.k8s.io/kube-scheduler:v1.31.0

kubectl get pods -n kube-system --watch
---

# Even though the deployment was scaled to 2, the number of PODs does not seem to increase. Investigate and fix the issue.
# Inspect the component responsible for managing deployments and replicasets.

kubectl describe deploy app

kubectl get po -n kube-system
      NAME                                   READY   STATUS             RESTARTS      AGE
      coredns-77d6fd4654-kcqp4               1/1     Running            0             18m
      coredns-77d6fd4654-rvrk4               1/1     Running            0             18m
      etcd-controlplane                      1/1     Running            0             18m
      kube-apiserver-controlplane            1/1     Running            0             18m
      kube-controller-manager-controlplane   0/1     CrashLoopBackOff   7 (88s ago)   12m ###
      kube-proxy-d4z2v                       1/1     Running            0             18m
      kube-scheduler-controlplane            1/1     Running            1 (13m ago)   12m


kubectl logs kube-controller-manager-controlplane -n kube-system
    I1130 13:46:21.720392       1 serving.go:386] Generated self-signed cert in-memory
    E1130 13:46:21.720495       1 run.go:72] "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"

# "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"

cat /etc/kubernetes/manifests/kube-controller-manager.yaml

vi /etc/kubernetes/manifests/kube-controller-manager.yaml


    spec:
      containers:
      - command:
        - kube-controller-manager
        - --allocate-node-cidrs=true
        - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
        - --bind-address=127.0.0.1
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --cluster-cidr=172.17.0.0/16
        - --cluster-name=kubernetes
        - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
        - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
        - --controllers=*,bootstrapsigner,tokencleaner
        - --kubeconfig=/etc/kubernetes/controller-manager-XXXX.conf   #### fix it to "controller-manager.conf"

---

# Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it's not happening.

k get deploy

k describe deploy
    Replicas:  3 desired | 2 updated | 2 total | 2 available | 0 unavailable

k get pods -n kube-system
    NAME                                   READY   STATUS             RESTARTS      AGE
    coredns-77d6fd4654-kcqp4               1/1     Running            0             38m
    coredns-77d6fd4654-rvrk4               1/1     Running            0             38m
    etcd-controlplane                      1/1     Running            0             38m
    kube-apiserver-controlplane            1/1     Running            0             38m
    kube-controller-manager-controlplane   0/1     CrashLoopBackOff   5 (16s ago)   3m27s  ####
    kube-proxy-d4z2v                       1/1     Running            0             38m
    kube-scheduler-controlplane            1/1     Running            1 (33m ago)   32m

k logs kube-controller-manager-controlplane -n kube-system
  I1130 14:09:46.717044       1 serving.go:386] Generated self-signed cert in-memory
  E1130 14:09:47.090943       1 run.go:72] "command failed" err="unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory"

ls /etc/kubernetes/pki/ca.crt  # this file should be mounted withon controller manager

cat /etc/kubernetes/manifests/kube-controller-manager.yaml

cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep /etc/kubernetes/pki/ca.crt
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt

vi /etc/kubernetes/manifests/kube-controller-manager.yaml


#Check the volume mount path in kube-controller-manager manifest file at /etc/kubernetes/manifests.

kubectl -n kube-system logs kube-controller-manager-controlplane
  I0916 13:17:27.452539       1 serving.go:348] Generated self-signed cert in-memory
  unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory

# It appears the path /etc/kubernetes/pki is not mounted from the controlplane to the kube-controller-manager pod. If we inspect the pod manifest file,
# we can see that the incorrect hostPath is used for the volume:

volumeMounts:
- mountPath: /etc/kubernetes/pki
  name: k8s-certs

# WRONG:
volumes:
- hostPath:
      path: /etc/kubernetes/WRONG-PKI-DIRECTORY  # change this
      type: DirectoryOrCreate
  name: k8s-certs

# CORRECT:
volumes:
- hostPath:
    path: /etc/kubernetes/pki
    type: DirectoryOrCreate
  name: k8s-certs

# Once the path is corrected, the pod will be recreated and our deployment should eventually scale up to 3 replicas.

---
# The cluster is broken again. Investigate and fix the issue.

kubectl get nodes
    NAME           STATUS     ROLES           AGE   VERSION
    controlplane   Ready      control-plane   22m   v1.31.0
    node01         NotReady   <none>          22m   v1.31.0

k describe node node01
        Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
        ----                 ------    -----------------                 ------------------                ------              -------
        NetworkUnavailable   False     Sat, 30 Nov 2024 15:59:35 +0000   Sat, 30 Nov 2024 15:59:35 +0000   FlannelIsUp         Flannel is running on this node
        MemoryPressure       Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        DiskPressure         Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        PIDPressure          Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
        Ready                Unknown   Sat, 30 Nov 2024 15:59:59 +0000   Sat, 30 Nov 2024 16:03:54 +0000   NodeStatusUnknown   Kubelet stopped posting node status.

ssh node01

service kubelet status

    ○ kubelet.service - kubelet: The Kubernetes Node Agent
         Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
        Drop-In: /usr/lib/systemd/system/kubelet.service.d
                 └─10-kubeadm.conf
         Active: inactive (dead) since Sat 2024-11-30 16:03:15 UTC; 22min ago
           Docs: https://kubernetes.io/docs/
        Process: 2583 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=0/SUCCESS)
       Main PID: 2583 (code=exited, status=0/SUCCESS)

    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770712    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables->
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770735    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modu>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770753    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770774    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"run\" (U>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770792    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-plug>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.770825    2583 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni\" (U>
    Nov 30 15:59:29 node01 kubelet[2583]: I1130 15:59:29.876611    2583 swap_util.go:54] "Running under a user namespace - tmpfs noswap is not supported"
    Nov 30 15:59:32 node01 kubelet[2583]: I1130 15:59:32.854506    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-wzpwz" po>
    Nov 30 15:59:33 node01 kubelet[2583]: I1130 15:59:33.163753    2583 kubelet_node_status.go:488] "Fast updating node status as it just became ready"
    Nov 30 16:03:15 node01 kubelet[2583]: I1130 16:03:15.153172    2583 dynamic_cafile_content.go:174] "Shutting down controller" name="client-ca-bundle::/etc/kubernetes/

service kubelet start

service kubelet status
          ● kubelet.service - kubelet: The Kubernetes Node Agent
               Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
              Drop-In: /usr/lib/systemd/system/kubelet.service.d
                       └─10-kubeadm.conf
               Active: active (running) since Sat 2024-11-30 16:26:49 UTC; 7s ago

---
# The cluster is broken again. Investigate and fix the issue.

      kubectl get nodes
      NAME           STATUS     ROLES           AGE   VERSION
      controlplane   Ready      control-plane   30m   v1.31.0
      node01         NotReady   <none>          30m   v1.31.0

k describe node node01

    Warning  InvalidDiskCapacity      3m                 kubelet          invalid capacity 0 on image filesystem
    Normal   NodeAllocatableEnforced  3m                 kubelet          Updated Node Allocatable limit across pods
    Normal   NodeHasSufficientMemory  3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasSufficientMemory
    Normal   NodeHasNoDiskPressure    3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasNoDiskPressure
    Normal   NodeHasSufficientPID     3m (x2 over 3m)    kubelet          Node node01 status is now: NodeHasSufficientPID
    Normal   NodeNotReady             34s (x2 over 25m)  node-controller  Node node01 status is now: NodeNotReady

ssh node01

service kubelet status
      ● kubelet.service - kubelet: The Kubernetes Node Agent
           Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
          Drop-In: /usr/lib/systemd/system/kubelet.service.d
                   └─10-kubeadm.conf
           Active: activating (auto-restart) (Result: exit-code) since Sat 2024-11-30 16:30:50 UTC; 3s ago
             Docs: https://kubernetes.io/docs/
          Process: 16744 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_>
         Main PID: 16744 (code=exited, status=1/FAILURE)

journalctl -u kubelet

      Nov 30 16:32:02 node01 kubelet[17394]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
      Nov 30 16:32:02 node01 kubelet[17394]: I1130 16:32:02.231123   17394 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote>
      Nov 30 16:32:02 node01 kubelet[17394]: E1130 16:32:02.233058   17394 run.go:72] "command failed" err="failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr>
      Nov 30 16:32:12 node01 kubelet[17504]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io>
      Nov 30 16:32:12 node01 kubelet[17504]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
      Nov 30 16:32:12 node01 kubelet[17504]: I1130 16:32:12.479762   17504 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote>
      Nov 30 16:32:12 node01 kubelet[17504]: E1130 16:32:12.481133   17504 run.go:72] "command failed" err="failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr

# unable to load client CA file /etc/kubernetes/pki/WRONG-CA-FILE.cr

# this is the kubeconfig file that kubelet uses to connect to master node
cat /etc/kubernetes/kubelet.conf    # no issue here:

      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: *****
          server: https://controlplane:6443
        name: default-cluster
      contexts:
      - context:
          cluster: default-cluster
          namespace: default
          user: default-auth
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: default-auth
        user:
          client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
          client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

cat /var/lib/kubelet/config.yaml  # this is kubelet configuration file

        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            cacheTTL: 0s
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/WRONG-CA-FILE.crt   #### this is obviously wrong, change to "ca.crt"
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: 0s
            cacheUnauthorizedTTL: 0s
        cgroupDriver: cgroupfs
        clusterDNS:
        - 172.20.0.10
        clusterDomain: cluster.local
        containerRuntimeEndpoint: ""
        cpuManagerReconcilePeriod: 0s
        evictionPressureTransitionPeriod: 0s
        fileCheckFrequency: 0s
        healthzBindAddress: 127.0.0.1
        healthzPort: 10248
        httpCheckFrequency: 0s
        imageMaximumGCAge: 0s
        imageMinimumGCAge: 0s
        kind: KubeletConfiguration
        logging:
          flushFrequency: 0
          options:
            json:
              infoBufferSize: "0"
            text:
              infoBufferSize: "0"
          verbosity: 0
        memorySwap: {}
        nodeStatusReportFrequency: 0s
        nodeStatusUpdateFrequency: 0s
        resolvConf: /run/systemd/resolve/resolv.conf
        rotateCertificates: true
        runtimeRequestTimeout: 0s
        shutdownGracePeriod: 0s
        shutdownGracePeriodCriticalPods: 0s
        staticPodPath: /etc/kubernetes/manifests
        streamingConnectionIdleTimeout: 0s
        syncFrequency: 0s
        volumeStatsAggPeriod: 0s

service kubelet restart

----
# The cluster is broken again. Investigate and fix the issue.

kubectl get nodes

kubectl describe nodes node01
        Normal   NodeHasSufficientMemory  2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
        Normal   NodeHasNoDiskPressure    2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
        Normal   NodeHasSufficientPID     2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientPID
        Normal   NodeNotReady             103s (x3 over 40m)     node-controller  Node node01 status is now: NodeNotReady


controlplane ~ ➜  ssh node01

node01 ~ ✖ service kubelet status
      ● kubelet.service - kubelet: The Kubernetes Node Agent
           Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
          Drop-In: /usr/lib/systemd/system/kubelet.service.d
                   └─10-kubeadm.conf
           Active: active (running) since Sat 2024-11-30 16:41:39 UTC; 5min ago
             Docs: https://kubernetes.io/docs/
         Main PID: 22489 (kubelet)
            Tasks: 23 (limit: 77143)
           Memory: 29.6M
           CGroup: /system.slice/kubelet.service
                   └─22489 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

      Nov 30 16:46:39 node01 kubelet[22489]: I1130 16:46:39.471151   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.472287   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.544272   22489 info.go:104] Failed to get disk map: could not parse device numbers from  for device md127
      Nov 30 16:46:39 node01 kubelet[22489]: E1130 16:46:39.664006   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:46:42 node01 kubelet[22489]: E1130 16:46:42.875781   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc9596162cde  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:46:44 node01 kubelet[22489]: W1130 16:46:44.786528   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:46:44 node01 kubelet[22489]: E1130 16:46:44.786607   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.169333   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:46:46 node01 kubelet[22489]: I1130 16:46:46.474193   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:46:46 node01 kubelet[22489]: E1130 16:46:46.475267   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"


journalctl -u kubelet

      Nov 30 16:48:03 node01 kubelet[22489]: E1130 16:48:03.504208   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      Nov 30 16:48:05 node01 kubelet[22489]: W1130 16:48:05.416942   22489 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      Nov 30 16:48:05 node01 kubelet[22489]: E1130 16:48:05.417018   22489 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      Nov 30 16:48:08 node01 kubelet[22489]: E1130 16:48:08.167814   22489 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc959a5f28e1  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      Nov 30 16:48:09 node01 kubelet[22489]: E1130 16:48:09.669318   22489 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.191833   22489 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      Nov 30 16:48:10 node01 kubelet[22489]: I1130 16:48:10.505617   22489 kubelet_node_status.go:72] "Attempting to register node" node="node01"
      Nov 30 16:48:10 node01 kubelet[22489]: E1130 16:48:10.506883   22489 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"

# port number should be 6443 not 6553 , "Unable to register node with API server" err="Post \"https://controlplane:6553/


node01 ~ ➜  cat /etc/kubernetes/kubelet.conf
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority-data: ****
          server: https://controlplane:6553   #### here is the issue, change it to "6443"
        name: default-cluster
      contexts:
      - context:
          cluster: default-cluster
          namespace: default
          user: default-auth
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: default-auth
        user:
          client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
          client-key: /var/lib/kubelet/pki/kubelet-client-current.pem


service kubelet restart
service kubelet status  # should be running with no errors now

----

# Network Troubleshooting

# Network Plugin in Kubernetes



There are several plugins available and these are some.

1. Weave Net:
To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
You can find details about the network plugins in the following documentation :
https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

2. Flannel :
 To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
Note: As of now flannel does not support kubernetes network policies.

3. Calico :
To install
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
Apply the manifest using the following command.
kubectl apply -f calico.yaml
Calico is said to have most advanced cni network plugin.

----

# **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab
# at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

k get svc -n triton
    NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    mysql         ClusterIP   10.100.190.141   <none>        3306/TCP         2m33s
    web-service   NodePort    10.111.38.4      <none>        8080:30081/TCP   2m33s

kubectl get po -n kube-system
    NAME                                   READY   STATUS    RESTARTS   AGE
    coredns-6f6b679f8f-mtxzv               1/1     Running   0          33m
    coredns-6f6b679f8f-qh272               1/1     Running   0          33m
    etcd-controlplane                      1/1     Running   0          33m
    kube-apiserver-controlplane            1/1     Running   0          33m
    kube-controller-manager-controlplane   1/1     Running   0          33m
    kube-proxy-smg6m                       1/1     Running   0          33m
    kube-scheduler-controlplane            1/1     Running   0          33m

kubectl logs -n kube-system kube-proxy-smg6m
      I1130 16:40:22.312587       1 server_linux.go:66] "Using iptables proxy"
      I1130 16:40:22.513570       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.121.133"]
      I1130 16:40:22.515026       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
      I1130 16:40:22.515101       1 conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
      I1130 16:40:22.515222       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
      I1130 16:40:22.515296       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
      E1130 16:40:22.515370       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
      I1130 16:40:22.566670       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
      I1130 16:40:22.566790       1 server_linux.go:169] "Using iptables Proxier"
      I1130 16:40:22.574112       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
      I1130 16:40:22.574888       1 server.go:483] "Version info" version="v1.31.0"
      I1130 16:40:22.574927       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
      I1130 16:40:22.589682       1 config.go:326] "Starting node config controller"
      I1130 16:40:22.592308       1 shared_informer.go:313] Waiting for caches to sync for node config
      I1130 16:40:22.599038       1 config.go:197] "Starting service config controller"
      I1130 16:40:22.599062       1 shared_informer.go:313] Waiting for caches to sync for service config
      I1130 16:40:22.599080       1 config.go:104] "Starting endpoint slice config controller"
      I1130 16:40:22.600353       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
      I1130 16:40:22.693393       1 shared_informer.go:320] Caches are synced for node config
      I1130 16:40:22.699693       1 shared_informer.go:320] Caches are synced for service config
      I1130 16:40:22.700885       1 shared_informer.go:320] Caches are synced for endpoint slice config

kubectl get po -A  | grep weave

curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -

----

# The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.

# Error: Environment Variables: DB_Host=mysql; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql:3306' (111 Connection refused)

kubectl get po -A
    NAMESPACE     NAME                                   READY   STATUS             RESTARTS        AGE
    kube-system   coredns-6f6b679f8f-mtxzv               1/1     Running            0               58m
    kube-system   coredns-6f6b679f8f-qh272               1/1     Running            0               58m
    kube-system   etcd-controlplane                      1/1     Running            0               58m
    kube-system   kube-apiserver-controlplane            1/1     Running            0               58m
    kube-system   kube-controller-manager-controlplane   1/1     Running            0               58m
    kube-system   kube-proxy-24rmj                       0/1     CrashLoopBackOff   9 (50s ago)     22m #######
    kube-system   kube-scheduler-controlplane            1/1     Running            0               58m
    kube-system   weave-net-gjxcx                        2/2     Running            0               23m
    triton        mysql                                  1/1     Running            4 (52s ago)     22m
    triton        webapp-mysql-d89894b4b-hrt4v           1/1     Running            2 (9m22s ago)   22m

kubectl logs -n kube-system kube-proxy-24rmj
    E1130 17:38:21.384015       1 run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"


kubectl get cm kube-proxy -n kube-system -o yaml
    apiVersion: v1
    data:
      config.conf: |-
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        bindAddress: 0.0.0.0
        bindAddressHardFail: false
        clientConnection:
          acceptContentTypes: ""
          burst: 0
          contentType: ""
          kubeconfig: /var/lib/kube-proxy/kubeconfig.conf   # this is correct address for kubeconfig
          qps: 0
        clusterCIDR: 10.244.0.0/16
        configSyncPeriod: 0s


kubectl get ds kube-proxy -n kube-system -o yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        labels:
          k8s-app: kube-proxy
        name: kube-proxy
        namespace: kube-system
        resourceVersion: "3485"
      spec:
        revisionHistoryLimit: 10
        selector:
          matchLabels:
            k8s-app: kube-proxy
        template:
          metadata:
            creationTimestamp: null
            labels:
              k8s-app: kube-proxy
          spec:
            containers:
            - command:
              - /usr/local/bin/kube-proxy
              - --config=/var/lib/kube-proxy/configuration.conf   # this is wrong and has to be changed to:  "/var/lib/kube-proxy/kubeconfig.conf"
              - --hostname-override=$(NODE_NAME)

kubectl edit daemonset kube-proxy -n kube-system

---

# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
 kubectl get nodes -o json > /opt/outputs/nodes.json

# Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json.
kubectl get node node01 -o json > /opt/outputs/node01.json

# Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

# Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt

# A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
kubectl config view --kubeconfig=/root/my-kube-config
kubectl config view --kubeconfig=/root/my-kube-config -o json
kubectl config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.users[*].name}' > /opt/outputs/users.txt
