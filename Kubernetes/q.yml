# Configuring security contexts for Pods and containers.

# Modify the existing Deployment named broker-deployment running in namespace quetzal so that its containers:
# 1) Run with user ID 30000 and
# 2) Privilege escalation is forbidden
# The broker-deployment is manifest file can be found at: my-deploy.yml

# vi my-deploy.yaml
# k apply -f my-deploy.yaml
# k get pods -n quetzal
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broker-deployment
  namespace: quetzal
  labels:
    app: broker-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: broker-deployment
  template:
    metadata:
      name: myapp-pod
      labels:
        app: broker-deployment
    spec:
      containers:
      - name: broker
        image: redis:alpine
        ports:
          - containerPort: 6379
        securityContext:
          runAsUser: 30000
          privilaged: false

#=============================================================================================
# Troubleshooting ingress issues.



#=============================================================================================
# NetworkPolicy to allow specific traffic.

# Update the Pod ckad00018-newpod in the ckad00018 namespace to use a NetworkPolicy allowing the Pod to send and receive traffic only to and from the pods web and db
# all required network policies have already been created, do not create or edit them!

# k get networkpolicy -n ckad00018
# k describe netpol my-current-netpol -n ckad00018  # check what tags the pod needs to send/recieve traffic from "web" and "db" pods

# k label pod ckad00018-newpod -n ckad00018 web-access=true
# k label pod ckad00018-newpod -n ckad00018 db-access=true


# pod front-end has label id: frontend
# pod api has label id: api
# create a network policy that pod frontend can only send the data to pod api
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np1
  namespace: venus
spec:
  podSelector:
    matchLabels:
      id: frontend          # label of the pods this policy should be applied on
  policyTypes:
  - Egress                  # we only want to control egress
  egress:
  # here we did not specify any port for egress
  - to:                     # 1st egress rule
    - podSelector:            # allow egress only to pods with api label
        matchLabels:
          id: api
  
  # these ports are NOT under egress, they are general ports for connection
  - ports:                  # 2nd egress rule
    - port: 53                # allow DNS UDP
      protocol: UDP
    - port: 53                # allow DNS TCP
      protocol: TCP

#=============================================================================================
# Canary deployment to distribute traffic in a 60-40 percent split.

# You are asked to prepare a Canary deployment for testing a new application release.
# A Service named krill-Service in the goshark namespace points to 5 pod created by the Deployment named current-krill-deployment
# Create an identical Deployment named canary-krill-deployment, in the same namespace.
# 2) Modify the Deployment so that:
# -A maximum number of 10 pods run in the goshark namespace.
# -40% of the krill-service 's traffic goes to the canary-krill-deployment pod(s)

# service
---
apiVersion: v1
kind: Service
metadata: 
  name: krill-Service
  namespace: goshark
spec:
  selector:
    app: krill-deployment
type: NodePort
ports:
  - port: 8080
    nodePort: 30000

# current-krill-deployment
# or just run
# k scale deploy current-krill-deployment --replicas=6 -n goshark
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: current-krill-deployment
  namespace: goshark
  labels:
    app: krill-deployment
spec:
  replicas: 6  # change from 5 to 6
  selector:
    matchLabels:
      app: krill-deployment
  template:
    metadata:
      name: myapp-pod
      labels:
        version: v1 ####
        app: krill-deployment
    spec:
      containers:
      - name: krill-deployment
        image: nginx

# canary-krill-deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: canary-krill-deployment
  namespace: goshark
  labels:
    app: krill-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: krill-deployment
  template:
    metadata:
      name: myapp-pod
      labels:
        version: v2 #####
        app: krill-deployment
    spec:
      containers:
      - name: krill-deployment
        image: nginx
#=============================================================================================
# daemonset
# used when you want to run one pod on each node, such as monitoring tools for example fluentD or proxy suchs calico or prometheus-exporters
# daemonset will create an many pods as there are nodes in cluster
# we don't set number of replicas on daemonset as it automatically deploys a pod on each node
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      tolerations:
        - effect: NoSchedule
          operator: Exists
      containers:
        - name: fluentd
          image: fluentd:v2.5.2
          resources:
            limits:
              memory: 256Mi
              cpu: "0.25"
          volumeMounts:
            - name: varlog
              mountPath: /var/log
      volumes:
        - name: varlog
          # daemoonset volumes use hostPath since the always run on specific node
          hostPath:
            path: /var/log

#=============================================================================================
# multi-container pod

# a container within the poller pod is hard-coded to connect to nginxsvc service on port 90, as this port changes to 5050
# an additional container needs to be added to the poller pod which adapts the container to connect to this new port, via ambassador container design

# update the nginxsvc service to serve on port 5050

# add an HAproxy container named haproxy bound to port 90 to the poller pod and deploy the enhanced pod.
# use the image haproxy and inject the configuration located at /opt/KDMC3323423/haproxy.cfg via a configmap
# named haproxy-config monuted to the container at /usr/local/etc/haproxy/haproxy.cfg

# update the original pod to connect to localhost instead of nginxsvc

# cat /opt/KDMC3323423/haproxy.cfg
# k create cm haproxy-config --from-file=/opt/KDMC3323423/haproxy.cfg
# k get cm cm haproxy-config

# another way to do it
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: haproxy-config
data:
  haproxy.cfg: |
    global
        log stdout  format raw  local0

    defaults
        log     global
        mode    tcp
        timeout connect 10s
        timeout client  1m
        timeout server  1m

    frontend http-in
        bind *:90
        default_backend servers

    backend servers
        server server1 nginxsvc:5050 maxconn 32


# edit the service
---
apiVersion: v1
kind: Service
metadata:
  name: nginxsvc
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 5050
      targetPort: 5050

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pod1
  name: poller
spec:
  containers:
  - name: poller-container
    image: poller:stable
    env:
      - name: NGINX_HOST
        value: "localhost"
      - name: NGINX_PORT
        value: "90"
  - name: haproxy
    image: haproxy
    volumeMounts:
      - name: haproxy-config
        mountPath: /usr/local/etc/haproxy/haproxy.cfg
        subPath: haproxy.cfg
  volumes:
    - name: haproxy-config
      configMap:
        name: haproxy-config

# we have a container that writes log files in format A and another that converts logs to format B, create a deployment
# that runs both containers 

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-container-app
spec:
  replicas: 2
  selector:
    matchLabels:
      run: app
  template:
    metadata:
      labels:
        run: app
    spec:
      containers:
      - name: web-one
        image: busybox:1.28
        command: ["/bin/sh", "-c", "while true; do echo 'i love cnfc' >> /tmp/log/input.log; sleep 10; done"]
        volumeMounts:
          - name: log-volume
            mountPath: /tmp/log
      - name: side-car
        image: busybox:1.28
        volumeMounts:
          - name: log-volume
            mountPath:  /tmp/log
      volumes:
        - name: log-volume
          emptyDir: {}

# Create a sidecar container named logger-con, image busybox:1.31.0 , which mounts the same volume and writes the content of cleaner.log to stdout, you can use the tail -f command
# Sidecar containers in K8s are initContainers with restartPolicy: Always
---
initContainers:
  - name: logger-con
    image: busybox:1.31.0
    restartPolicy: Always # sidecar containers should have always restart policy
    command: ["/bin/sh", "-c", "tail -f /var/log/cleaner/cleaner.log"]
    volumeMounts:
      - name: logs
        mountPath: /var/log/cleaner

# you can't mount a volume to root of a container!  mountPath: /  => will fail
#=============================================================================================
# labels and annotations
# Team Sunny needs to identify some of their Pods in namespace sun. They ask you to add a new label protected: true to all Pods 
# with an existing label type: worker or type: runner. Also add an annotation protected: do not delete this pod to all Pods having 
# the new label protected: true.

k -n sun label pod -l "type in (worker,runner)" protected=true

k -n sun label pod -l "type in (worker,runner)" protected=true

#=============================================================================================

# a pod is unreachable due to faoling livenessprobe
# find the broken pod and store its name and namespace to file /opt/KDBOD23423/broken.txt in format namespace/pod
# store associated error events to file /opt/KDBOD23423/error.txt
# fix the issue

# k get po -A | grep "qa|prod|dev|lab"
# k get po -A | grep -i failed
# echo "mynamespace/my-pod" > /opt/KDBOD23423/error.txt

# k describe pod my-pod -n mynamespace | grep -i error > /opt/KDBOD23423/error.txt

# based on error events we can fixed the issue, it can be image is wrong or pod doesnt have enough resources (OOM Kill) or livenessProbe not set correctly

#=============================================================================================

# a deployment is failing on the cluster die to an incorrect image being used, locate the deployment and fix the problem

# k get deploy  # this might NOT show that why deployment is failing
# k get po 

# k edit deploy my-ngnix-deployment 
# # change the image to correct value

# k get po
#=============================================================================================
# helm

# Delete release internal-issue-report-apiv1
helm -n mercury uninstall internal-issue-report-apiv1

# Upgrade release internal-issue-report-apiv2 to any newer version of chart bitnami/nginx available
helm -n mercury upgrade internal-issue-report-apiv2 bitnami/nginx

# Install a new release internal-issue-report-apache of chart bitnami/apache. The Deployment should have two replicas, set these via Helm-values during install
helm show values bitnami/apache | grep -i count
helm -n mercury install internal-issue-report-apache bitnami/apache --set replicaCount=2

# There seems to be a broken release, stuck in pending-install state. Find it and delete it
# By default releases in pending-upgrade state aren't listed, but we can show all to find and delete the broken release
helm -n mercury ls -a
helm -n mercury uninstall internal-issue-report-daniel
#=============================================================================================

# get specific label or annotation from a pod from a list of pods
k -n saturn describe pod | grep -i my-happy-shop
k -n saturn get pod -o yaml | grep -i my-happy-shop

# when you have a list of rollouts and current one isn't working, only way to go back to a working one is to go back one by one to see which one worked before

how to test a working service from a temporary pod

find the service endpoint
k -n pluto describe svc project-plt-svc | grep -i endpoints
k run temp --image=nginx:alpine -- curl http://project-plt-svc:3333


# how to test curl on a pod without svc
k get po -o wide # get pod's ip address
k run temp --image=nginx:alpine -- curl 10.44.0.78
k logs temp
#=============================================================================================

# PV doesn't have a namespace
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: earth-project-earthflower-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /Volumes/Data

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: earth-project-earthflower-pvc
  namespace: earth
spec:
  resources:
    requests:
      storage: 2Gi
  accessModes:
    - ReadWriteOnce

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: project-earthflower
  namespace: earth
spec:
  replicas: 1
  selector:
    matchLabels:
      app: project-earthflower
  template:
    metadata:
      labels:
        app: project-earthflower
    spec:
      volumes:
        - name: my-vol
          persistentVolumeClaim:
            claimName: earth-project-earthflower-pvc
      containers:
      - image: httpd:2.4.41-alpine
        name: container
        volumeMounts:
          - name: my-vol
            mountPath: /tmp/project-data

# to get pvc events 
k -n earth describe pvc earth-project-earthflower-pvc # Event section is at end
# manually copy it to file /opt/course/13/pvc-126-reason

#=============================================================================================

# This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain
# storageclass doesnt have a namespace
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: moon-retain
provisioner: moon-retainer
reclaimPolicy: Retain
