# the cluster is broken again, investigate and fix the issue
kubectl get nodes
kubectl describe nodes node01
        # Normal   NodeHasSufficientMemory  2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
        # Normal   NodeHasNoDiskPressure    2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
        # Normal   NodeHasSufficientPID     2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientPID
        # Normal   NodeNotReady             103s (x3 over 40m)     node-controller  Node node01 status is now: NodeNotReady

ssh node01
service kubelet status

      # ● kubelet.service - kubelet: The Kubernetes Node Agent
      #      Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
      #     Drop-In: /usr/lib/systemd/system/kubelet.service.d
      #              └─10-kubeadm.conf
      #      Active: active (running) since Sat 2024-11-30 16:41:39 UTC; 5min ago
      #        Docs: https://kubernetes.io/docs/
      #    Main PID: 22489 (kubelet)
      #       Tasks: 23 (limit: 77143)
      #      Memory: 29.6M
      #      CGroup: /system.slice/kubelet.service
      #              └─22489 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

      #  kubelet_node_status.go:72] "Attempting to register node" node="node01"
      #  kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      #  info.go:104] Failed to get disk map: could not parse device numbers from  for device md127
      #  eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      #  event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc9596162cde  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      #  reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      #  reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      #  controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      #  kubelet_node_status.go:72] "Attempting to register node" node="node01"
      #  kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"

journalctl -u kubelet
      # kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      # reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      # reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      # event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc959a5f28e1  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      # eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      # controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      # kubelet_node_status.go:72] "Attempting to register node" node="node01"
      # kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"   

# port number should be 6443 not 6553, "Unable to register node with API server" err="Post \"https://controlplane:6553/"

cat /etc/kubernetes/kubelet.conf  # this is the kubeconfig file that kubelet uses to connect to kube-apiserver

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: **** # CA 
    server: https://controlplane:6553  #### here is the issue, change it to "6443"
  name: default cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-key.pem

service kubelet restart
service kubelet status  # should be running with no errors now


# Network Troubleshooting
# Network Plugin in Kubernetes

# There are several plugins available and these are some.

# 1. Weave Net:
# To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
# You can find details about the network plugins in the following documentation :
# https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

# 2. Flannel :
#  To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
# Note: As of now flannel does not support kubernetes network policies.

# 3. Calico :
# To install
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
# Apply the manifest using the following command.
kubectl apply -f calico.yaml
# Calico is said to have most advanced cni network plugin.


# **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab
# at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
k get svc -n triton
      # NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
      # mysql         ClusterIP   10.100.190.141   <none>        3306/TCP         2m33s
      # web-service   NodePort    10.111.38.4      <none>        8080:30081/TCP   2m33s

kubectl get po -n kube-system
    # NAME                                   READY   STATUS    RESTARTS   AGE
    # coredns-6f6b679f8f-mtxzv               1/1     Running   0          33m
    # coredns-6f6b679f8f-qh272               1/1     Running   0          33m
    # etcd-controlplane                      1/1     Running   0          33m
    # kube-apiserver-controlplane            1/1     Running   0          33m
    # kube-controller-manager-controlplane   1/1     Running   0          33m
    # kube-proxy-smg6m                       1/1     Running   0          33m
    # kube-scheduler-controlplane            1/1     Running   0          33m

kubectl logs -n kube-system kube-proxy-smg6m
      # server_linux.go:66] "Using iptables proxy"
      # server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.121.133"]
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
      # conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
      # server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
      # "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
      # server_linux.go:169] "Using iptables Proxier"
      # proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
      # server.go:483] "Version info" version="v1.31.0"
      # server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
      # config.go:326] "Starting node config controller"
      # shared_informer.go:313] Waiting for caches to sync for node config
      # config.go:197] "Starting service config controller"
      # shared_informer.go:313] Waiting for caches to sync for service config
      # config.go:104] "Starting endpoint slice config controller"
      # shared_informer.go:313] Waiting for caches to sync for endpoint slice config
      # shared_informer.go:320] Caches are synced for node config
      # shared_informer.go:320] Caches are synced for service config
      # shared_informer.go:320] Caches are synced for endpoint slice config

# there is no network plugin, we need to install one:
kubectl get po -A | grep weave
curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -


# The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
# Error: Environment Variables: DB_Host=mysql; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql:3306' (111 Connection refused)

kubectl get po -A
    # NAMESPACE     NAME                                   READY   STATUS             RESTARTS        AGE
    # kube-system   coredns-6f6b679f8f-mtxzv               1/1     Running            0               58m
    # kube-system   coredns-6f6b679f8f-qh272               1/1     Running            0               58m
    # kube-system   etcd-controlplane                      1/1     Running            0               58m
    # kube-system   kube-apiserver-controlplane            1/1     Running            0               58m
    # kube-system   kube-controller-manager-controlplane   1/1     Running            0               58m
    # kube-system   kube-proxy-24rmj                       0/1     CrashLoopBackOff   9 (50s ago)     22m   #######
    # kube-system   kube-scheduler-controlplane            1/1     Running            0               58m
    # kube-system   weave-net-gjxcx                        2/2     Running            0               23m
    # triton        mysql                                  1/1     Running            4 (52s ago)     22m
    # triton        webapp-mysql-d89894b4b-hrt4v           1/1     Running            2 (9m22s ago)   22m

kubectl logs -n kube-system kube-proxy-24rmj
    # run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"

kubectl get cm kube-proxy -n kube-system -o yaml
    # apiVersion: v1
    # data:
    #   config.conf: |-
    #     apiVersion: kubeproxy.config.k8s.io/v1alpha1
    #     bindAddress: 0.0.0.0
    #     bindAddressHardFail: false
    #     clientConnection:
    #       acceptContentTypes: ""
    #       burst: 0
    #       contentType: ""
    #       kubeconfig: /var/lib/kube-proxy/kubeconfig.conf   # this is correct address for kubeconfig
    #       qps: 0
    #     clusterCIDR: 10.244.0.0/16
    #     configSyncPeriod: 0s

kubectl get daemonset kube-proxy -n kube-system -o yaml

      # apiVersion: apps/v1
      # kind: DaemonSet
      # metadata:
      #   labels:
      #     k8s-app: kube-proxy
      #   name: kube-proxy
      #   namespace: kube-system
      # spec:
      #   selector:
      #     matchLabels:
      #       k8s-app: kube-proxy
      #   template:
      #     metadata:
      #       labels:
      #         k8s-app: kube-proxy
      #     spec:
      #       containers:
      #       - command:
      #         - /usr/local/bin/kube-proxy
      #         - --config=/var/lib/kube-proxy/configuration.conf   # this is wrong and has to be changed to:  "/var/lib/kube-proxy/kubeconfig.conf"
      #         - --hostname-override=$(NODE_NAME)

kubectl edit daemonset kube-proxy -n kube-system


# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json
kubectl get nodes -o json > /opt/outputs/nodes.json

# Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json
kubectl get node node01 -o json > /opt/outputs/node01.json

# Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

# Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
kubectl get nodes -o=jsonpath='{.items[*].status.nodeinfo.osImage}' > /opt/outputs/node_os.txt

# A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
kubectl config view --kubeconfig=/root/my-kube-config
kubectl config view --kubeconfig=/root/my-kube-config -o json
kubectl config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.users[*].name}' > /opt/outputs/users.txt



### Upgrade the current version of kubernetes from 1.30.0 to 1.31.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the controlplane node. 
# To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node.
# upgrade controlplane node first and drain node node01 before upgrading it. pods for gold-nginx should run on the controlplane node subsequently

# on controlpnae node
vi /etc/apt/source.list.d/kubernetes.list
# Update the version in the URL to the next available minor release, i.e v1.31.
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring-gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
kubectl drain controlplane --ignore-daemonsets
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.31.0-1.1
kubeadm upgrade plan v1.31.0
kubeadm upgrade apply v1.31.0
# upgrade kubelet
apt-get install kubelet=1.31.0-1.1
systemctl daemon-reload
systemctl restart kubelet
kubectl uncordon controlplane
# Before draining node01, if the controlplane gets taint during an upgrade, we have to remove it.
kubectl describe node controlplane | grep -i taint
kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-
kubectl describe node controlplane | grep -i taint
# drain and upgrade node01
kubectl drain node01 --ignore-daemonsets
# SSH to the node01 and perform the below steps as follows: -
vi /etc/apt/source.list.d/kubernetes.list
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.kBs.10/core:/stable:/v1.31/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.31.0-1.1
kubeadm upgrade node
# upgrade kubelet
apt-get install kubelet=1.31.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# exit back to controlplane node
kubectl uncordon node01
kubectl get pods -o wide | grep gold # make sure this is scheduled on a node
