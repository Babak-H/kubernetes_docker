# the cluster is broken again, investigate and fix the issue
kubectl get nodes
kubectl describe nodes node01
        # Normal   NodeHasSufficientMemory  2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
        # Normal   NodeHasNoDiskPressure    2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
        # Normal   NodeHasSufficientPID     2m47s (x2 over 2m48s)  kubelet          Node node01 status is now: NodeHasSufficientPID
        # Normal   NodeNotReady             103s (x3 over 40m)     node-controller  Node node01 status is now: NodeNotReady

ssh node01
service kubelet status

      # ● kubelet.service - kubelet: The Kubernetes Node Agent
      #      Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
      #     Drop-In: /usr/lib/systemd/system/kubelet.service.d
      #              └─10-kubeadm.conf
      #      Active: active (running) since Sat 2024-11-30 16:41:39 UTC; 5min ago
      #        Docs: https://kubernetes.io/docs/
      #    Main PID: 22489 (kubelet)
      #       Tasks: 23 (limit: 77143)
      #      Memory: 29.6M
      #      CGroup: /system.slice/kubelet.service
      #              └─22489 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10

      #  kubelet_node_status.go:72] "Attempting to register node" node="node01"
      #  kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      #  info.go:104] Failed to get disk map: could not parse device numbers from  for device md127
      #  eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      #  event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc9596162cde  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      #  reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      #  reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://controlplane:6553/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      #  controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      #  kubelet_node_status.go:72] "Attempting to register node" node="node01"
      #  kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"

journalctl -u kubelet
      # kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"
      # reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0": dial tcp 192.168.227.122:6553: connect: connection refused
      # reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://controlplane:6553/api/v1/nodes?fieldSelector=metadata.name%3Dnode01&limit=500&resourceVersion=0\": dial tcp 192.168.227.122:6553: connect: connection refused" logger="UnhandledError"
      # event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://controlplane:6553/api/v1/namespaces/default/events\": dial tcp 192.168.227.122:6553: connect: connection refused" event="&Event{ObjectMeta:{node01.180ccc959a5f28e1  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:O>
      # eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"node01\" not found"
      # controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://controlplane:6553/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node01?timeout=10s\": dial tcp 192.168.227.122:6553: connect: connection refused" interval="7s"
      # kubelet_node_status.go:72] "Attempting to register node" node="node01"
      # kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://controlplane:6553/api/v1/nodes\": dial tcp 192.168.227.122:6553: connect: connection refused" node="node01"   

# port number should be 6443 not 6553, "Unable to register node with API server" err="Post \"https://controlplane:6553/"

cat /etc/kubernetes/kubelet.conf  # this is the kubeconfig file that kubelet uses to connect to kube-apiserver

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: **** # CA 
    server: https://controlplane:6553  #### here is the issue, change it to "6443"
  name: default cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-key.pem

service kubelet restart
service kubelet status  # should be running with no errors now


# Network Troubleshooting
# Network Plugin in Kubernetes

# There are several plugins available and these are some.

# 1. Weave Net:
# To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
# You can find details about the network plugins in the following documentation :
# https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

# 2. Flannel :
#  To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
# Note: As of now flannel does not support kubernetes network policies.

# 3. Calico :
# To install
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O
# Apply the manifest using the following command.
kubectl apply -f calico.yaml
# Calico is said to have most advanced cni network plugin.


# **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab
# at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
k get svc -n triton
      # NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
      # mysql         ClusterIP   10.100.190.141   <none>        3306/TCP         2m33s
      # web-service   NodePort    10.111.38.4      <none>        8080:30081/TCP   2m33s

kubectl get po -n kube-system
    # NAME                                   READY   STATUS    RESTARTS   AGE
    # coredns-6f6b679f8f-mtxzv               1/1     Running   0          33m
    # coredns-6f6b679f8f-qh272               1/1     Running   0          33m
    # etcd-controlplane                      1/1     Running   0          33m
    # kube-apiserver-controlplane            1/1     Running   0          33m
    # kube-controller-manager-controlplane   1/1     Running   0          33m
    # kube-proxy-smg6m                       1/1     Running   0          33m
    # kube-scheduler-controlplane            1/1     Running   0          33m

kubectl logs -n kube-system kube-proxy-smg6m
      # server_linux.go:66] "Using iptables proxy"
      # server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.121.133"]
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
      # conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
      # conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
      # server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
      # "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
      # server_linux.go:169] "Using iptables Proxier"
      # proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
      # server.go:483] "Version info" version="v1.31.0"
      # server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
      # config.go:326] "Starting node config controller"
      # shared_informer.go:313] Waiting for caches to sync for node config
      # config.go:197] "Starting service config controller"
      # shared_informer.go:313] Waiting for caches to sync for service config
      # config.go:104] "Starting endpoint slice config controller"
      # shared_informer.go:313] Waiting for caches to sync for endpoint slice config
      # shared_informer.go:320] Caches are synced for node config
      # shared_informer.go:320] Caches are synced for service config
      # shared_informer.go:320] Caches are synced for endpoint slice config

# there is no network plugin, we need to install one:
kubectl get po -A | grep weave
curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -


# The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
# Error: Environment Variables: DB_Host=mysql; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql:3306' (111 Connection refused)

kubectl get po -A
    # NAMESPACE     NAME                                   READY   STATUS             RESTARTS        AGE
    # kube-system   coredns-6f6b679f8f-mtxzv               1/1     Running            0               58m
    # kube-system   coredns-6f6b679f8f-qh272               1/1     Running            0               58m
    # kube-system   etcd-controlplane                      1/1     Running            0               58m
    # kube-system   kube-apiserver-controlplane            1/1     Running            0               58m
    # kube-system   kube-controller-manager-controlplane   1/1     Running            0               58m
    # kube-system   kube-proxy-24rmj                       0/1     CrashLoopBackOff   9 (50s ago)     22m   #######
    # kube-system   kube-scheduler-controlplane            1/1     Running            0               58m
    # kube-system   weave-net-gjxcx                        2/2     Running            0               23m
    # triton        mysql                                  1/1     Running            4 (52s ago)     22m
    # triton        webapp-mysql-d89894b4b-hrt4v           1/1     Running            2 (9m22s ago)   22m

kubectl logs -n kube-system kube-proxy-24rmj
    # run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"

kubectl get cm kube-proxy -n kube-system -o yaml
    # apiVersion: v1
    # data:
    #   config.conf: |-
    #     apiVersion: kubeproxy.config.k8s.io/v1alpha1
    #     bindAddress: 0.0.0.0
    #     bindAddressHardFail: false
    #     clientConnection:
    #       acceptContentTypes: ""
    #       burst: 0
    #       contentType: ""
    #       kubeconfig: /var/lib/kube-proxy/kubeconfig.conf   # this is correct address for kubeconfig
    #       qps: 0
    #     clusterCIDR: 10.244.0.0/16
    #     configSyncPeriod: 0s

kubectl get daemonset kube-proxy -n kube-system -o yaml

      # apiVersion: apps/v1
      # kind: DaemonSet
      # metadata:
      #   labels:
      #     k8s-app: kube-proxy
      #   name: kube-proxy
      #   namespace: kube-system
      # spec:
      #   selector:
      #     matchLabels:
      #       k8s-app: kube-proxy
      #   template:
      #     metadata:
      #       labels:
      #         k8s-app: kube-proxy
      #     spec:
      #       containers:
      #       - command:
      #         - /usr/local/bin/kube-proxy
      #         - --config=/var/lib/kube-proxy/configuration.conf   # this is wrong and has to be changed to:  "/var/lib/kube-proxy/kubeconfig.conf"
      #         - --hostname-override=$(NODE_NAME)

kubectl edit daemonset kube-proxy -n kube-system


# Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json
kubectl get nodes -o json > /opt/outputs/nodes.json

# Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json
kubectl get node node01 -o json > /opt/outputs/node01.json

# Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}' > /opt/outputs/node_names.txt

# Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt
kubectl get nodes -o=jsonpath='{.items[*].status.nodeinfo.osImage}' > /opt/outputs/node_os.txt

# A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt
kubectl config view --kubeconfig=/root/my-kube-config
kubectl config view --kubeconfig=/root/my-kube-config -o json
kubectl config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.users[*].name}' > /opt/outputs/users.txt



### Upgrade the current version of kubernetes from 1.30.0 to 1.31.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the controlplane node. 
# To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node.
# upgrade controlplane node first and drain node node01 before upgrading it. pods for gold-nginx should run on the controlplane node subsequently

# on controlpnae node
vi /etc/apt/source.list.d/kubernetes.list
# Update the version in the URL to the next available minor release, i.e v1.31.
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring-gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
kubectl drain controlplane --ignore-daemonsets
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.31.0-1.1
kubeadm upgrade plan v1.31.0
kubeadm upgrade apply v1.31.0
# upgrade kubelet
apt-get install kubelet=1.31.0-1.1
systemctl daemon-reload
systemctl restart kubelet
kubectl uncordon controlplane
# Before draining node01, if the controlplane gets taint during an upgrade, we have to remove it.
kubectl describe node controlplane | grep -i taint
kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-
kubectl describe node controlplane | grep -i taint
# drain and upgrade node01
kubectl drain node01 --ignore-daemonsets
# SSH to the node01 and perform the below steps as follows: -
vi /etc/apt/source.list.d/kubernetes.list
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.kBs.10/core:/stable:/v1.31/deb/ /
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.31.0-1.1
kubeadm upgrade node
# upgrade kubelet
apt-get install kubelet=1.31.0-1.1
systemctl daemon-reload
systemctl restart kubelet
# exit back to controlplane node
kubectl uncordon node01
kubectl get pods -o wide | grep gold # make sure this is scheduled on a node

# isntall docker on nodes for kubertnets installation
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get update
sudo apt-get install -y docker-ce
sudo systemctl status docker
sudo docker run hello-world


# ask chatgpt: kube-schduler only connects to kube-api server. does it need to have a private key certificate file? if yes why?
# please visualize it with an image/diagram

#//////////////////////////////////////////////////////////////////////////////
---
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        labels:
          component: kube-apiserver
      spec:
        # Node selector to ensure the pod runs on the master node
        nodeSelector:
          node-role.kubernetes.io/master: ""

        # Containers section defines the kube-apiserver container
        containers:
        - name: kube-apiserver
          image: k8s.gcr.io/kube-apiserver:v1.22.0
          command:
          - kube-apiserver
          # Flags for configuring the API server
          - --advertise-address=192.168.0.1
          - --allow-privileged=true
          - --authorization-mode=Node,RBAC
          - --client-ca-file=/etc/kubernetes/pki/ca.crt
          - --enable-admission-plugins=NodeRestriction
          - --etcd-servers=https://127.0.0.1:2379
          - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
          - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
          - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
          - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
          - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
          - --service-account-key-file=/etc/kubernetes/pki/sa.pub
          - --service-cluster-ip-range=10.96.0.0/12
          - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
          - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
          ports:
          - containerPort: 6443
            hostPort: 6443
            name: https
          volumeMounts:
          # Mounting necessary certificates and keys
          - mountPath: /etc/kubernetes/pki
            name: pki
            readOnly: true
        # Volumes section for mounting certificates and keys
        volumes:
        - name: pki
          hostPath:
            path: /etc/kubernetes/pki
            type: DirectoryOrCreate
        # Security context to run the container as a non-root user
        securityContext:
          runAsUser: 1000
          runAsGroup: 3000
          fsGroup: 2000

# Node Selector: Ensures that the kube-apiserver pod runs on a node labeled as a master node.
# Containers: Defines the container that runs the kube-apiserver binary. The command section includes various flags to configure the API server, such as authentication, authorization, and connection to the etcd database.
# Ports: Specifies the ports that the API server listens on, both inside the container and on the host.
# Volume Mounts and Volumes: These sections are used to mount necessary certificates and keys into the container, which are required for secure communication.
# Security Context: Specifies the user and group IDs under which the container should run, enhancing security by avoiding running as the root user.
# This is a basic example and may need to be adjusted based on your specific Kubernetes setup and version.


---
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-scheduler
      namespace: kube-system
      labels:
        component: kube-scheduler
    spec:
      # Node selector to ensure the pod runs on the master node
      nodeSelector:
        node-role.kubernetes.io/master: ""

      # Containers section defines the kube-scheduler container
      containers:
      - name: kube-scheduler
        image: k8s.gcr.io/kube-scheduler:v1.22.0
        command:
        - kube-scheduler
        # Flags for configuring the scheduler
        - --kubeconfig=/etc/kubernetes/scheduler.conf
        - --leader-elect=true
        - --bind-address=127.0.0.1
        ports:
        - containerPort: 10251
          hostPort: 10251
          name: http
        volumeMounts:
        # Mounting the kubeconfig file for the scheduler
        - mountPath: /etc/kubernetes
          name: kubeconfig
          readOnly: true

      # Volumes section for mounting the kubeconfig file
      volumes:
      - name: kubeconfig
        hostPath:
          path: /etc/kubernetes
          type: DirectoryOrCreate

      # Security context to run the container as a non-root user
      securityContext:
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000

# Node Selector: Ensures that the kube-scheduler pod runs on a node labeled as a master node.
# Containers: Defines the container that runs the kube-scheduler binary. The command section includes flags to configure the scheduler, such as specifying the kubeconfig file for cluster access and enabling leader election for high availability.
# Ports: Specifies the ports that the scheduler listens on, both inside the container and on the host. The default port for the scheduler is 10251.
# Volume Mounts and Volumes: These sections are used to mount the kubeconfig file into the container, which is required for the scheduler to communicate with the Kubernetes API server.
# Security Context: Specifies the user and group IDs under which the container should run, enhancing security by avoiding running as the root user.

---
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-controller-manager
      namespace: kube-system
      labels:
        component: kube-controller-manager
    spec:
      # Node selector to ensure the pod runs on the master node
      nodeSelector:
        node-role.kubernetes.io/master: ""

      # Containers section defines the kube-controller-manager container
      containers:
      - name: kube-controller-manager
        image: k8s.gcr.io/kube-controller-manager:v1.22.0
        command:
        - kube-controller-manager
        # Flags for configuring the controller manager
        - --kubeconfig=/etc/kubernetes/controller-manager.conf
        - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
        - --root-ca-file=/etc/kubernetes/pki/ca.crt
        - --leader-elect=true
        - --allocate-node-cidrs=true
        - --cluster-cidr=10.244.0.0/16
        ports:
        - containerPort: 10252
          hostPort: 10252
          name: http
        volumeMounts:
        # Mounting necessary configuration and key files
        - mountPath: /etc/kubernetes
          name: kubeconfig
          readOnly: true
        - mountPath: /etc/kubernetes/pki
          name: pki
          readOnly: true
      # Volumes section for mounting configuration and key files
      volumes:
      - name: kubeconfig
        hostPath:
          path: /etc/kubernetes
          type: DirectoryOrCreate
      - name: pki
        hostPath:
          path: /etc/kubernetes/pki
          type: DirectoryOrCreate
      # Security context to run the container as a non-root user
      securityContext:
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000

# Node Selector: Ensures that the kube-controller-manager pod runs on a node labeled as a master node.
# Containers: Defines the container that runs the kube-controller-manager binary. The command section includes flags to configure the controller manager, such as specifying the kubeconfig file for cluster access, service account key files, and enabling leader election for high availability.
# Ports: Specifies the ports that the controller manager listens on, both inside the container and on the host. The default port for the controller manager is 10252.
# Volume Mounts and Volumes: These sections are used to mount necessary configuration and key files into the container, which are required for the controller manager to communicate with the Kubernetes API server and manage resources.
# Security Context: Specifies the user and group IDs under which the container should run, enhancing security by avoiding running as the root user.

---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: kube-proxy
      namespace: kube-system
      labels:
        k8s-app: kube-proxy
    spec:
      selector:
        matchLabels:
          k8s-app: kube-proxy
      template:
        metadata:
          labels:
            k8s-app: kube-proxy
        spec:
          # Service account for kube-proxy
          serviceAccountName: kube-proxy

          # Containers section defines the kube-proxy container
          containers:
          - name: kube-proxy
            image: k8s.gcr.io/kube-proxy:v1.22.0
            command:
            - /usr/local/bin/kube-proxy
            # Flags for configuring kube-proxy
            - --config=/var/lib/kube-proxy/config.conf
            securityContext:
              privileged: true
            volumeMounts:
            # Mounting necessary configuration files
            - mountPath: /var/lib/kube-proxy
              name: kube-proxy
            - mountPath: /etc/ssl/certs
              name: ssl-certs
              readOnly: true
          # Volumes section for mounting configuration files
          volumes:
          - name: kube-proxy
            configMap:
              name: kube-proxy
          - name: ssl-certs
            hostPath:
              path: /etc/ssl/certs
              type: Directory
          # Tolerations to allow kube-proxy to run on all nodes
          tolerations:
          - operator: Exists

# DaemonSet: Ensures that a kube-proxy pod runs on each node in the cluster.
# Service Account: Specifies the service account under which the kube-proxy runs. This is necessary for accessing the Kubernetes API.
# Containers: Defines the container that runs the kube-proxy binary. The command section includes flags to configure kube-proxy, such as specifying the configuration file.
# Security Context: The privileged flag is set to true to allow kube-proxy to modify network settings on the host.
# Volume Mounts and Volumes: These sections are used to mount necessary configuration files and SSL certificates into the container. The configuration is typically provided via a ConfigMap.
# Tolerations: Allows the kube-proxy to run on all nodes, including those with taints, by tolerating all taints with the Exists operator.

---
    apiVersion: v1
    kind: Pod
    metadata:
      name: etcd
      namespace: kube-system
      labels:
        component: etcd
    spec:
      # Node selector to ensure the pod runs on the master node
      nodeSelector:
        node-role.kubernetes.io/master: ""
      # Containers section defines the etcd container
      containers:
      - name: etcd
        image: quay.io/coreos/etcd:v3.5.0
        command:
        - /usr/local/bin/etcd
        # Flags for configuring etcd
        - --name=etcd0
        - --data-dir=/var/lib/etcd
        - --listen-client-urls=https://0.0.0.0:2379
        - --advertise-client-urls=https://0.0.0.0:2379
        - --listen-peer-urls=https://0.0.0.0:2380
        - --initial-advertise-peer-urls=https://0.0.0.0:2380
        - --initial-cluster=etcd0=https://0.0.0.0:2380
        - --initial-cluster-state=new
        - --client-cert-auth=true
        - --trusted-ca-file=/etc/etcd/pki/ca.crt
        - --cert-file=/etc/etcd/pki/etcd.crt
        - --key-file=/etc/etcd/pki/etcd.key
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: peer
        volumeMounts:
        # Mounting necessary data and certificate files
        - mountPath: /var/lib/etcd
          name: etcd-data
        - mountPath: /etc/etcd/pki
          name: etcd-certs
          readOnly: true
      # Volumes section for mounting data and certificate files
      volumes:
      - name: etcd-data
        hostPath:
          path: /var/lib/etcd
          type: DirectoryOrCreate
      - name: etcd-certs
        hostPath:
          path: /etc/etcd/pki
          type: DirectoryOrCreate
      # Security context to run the container as a non-root user
      securityContext:
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000

# Node Selector: Ensures that the etcd pod runs on a node labeled as a master node.
# Containers: Defines the container that runs the etcd binary. The command section includes flags to configure etcd, such as specifying the node name, data directory, and network settings for client and peer communication.
# Ports: Specifies the ports that etcd listens on for client and peer communication.
# Volume Mounts and Volumes: These sections are used to mount necessary data directories and certificate files into the container. The data directory is where etcd stores its data, and the certificates are used for secure communication.
# Security Context: Specifies the user and group IDs under which the container should run, enhancing security by avoiding running as the root user.

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    component: kube-proxy
    tier: node
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration

    # Proxy mode: choose between 'iptables' or 'ipvs'
    mode: "iptables"
    # CIDR range for the cluster's pod network
    clusterCIDR: "10.244.0.0/16"
    clientConnection:
      # Path to the kubeconfig file for API server connection
      kubeconfig: "/var/lib/kube-proxy/kubeconfig.conf"
    iptables:
      # Whether to masquerade all traffic
      masqueradeAll: false
      # Bit of the iptables mark space to use for SNAT
      masqueradeBit: 14
      # Minimum interval between iptables rule syncs
      minSyncPeriod: "0s"
      # Interval between full iptables rule syncs
      syncPeriod: "30s"
    ipvs:
      # Minimum interval between IPVS rule syncs
      minSyncPeriod: "0s"
      # IPVS scheduler (e.g., 'rr' for round-robin)
      scheduler: "rr"
      # Interval between full IPVS rule syncs
      syncPeriod: "30s"
    # Address and port for the metrics server
    metricsBindAddress: "0.0.0.0:10249"
    # IP addresses to use for NodePort services
    nodePortAddresses:
      - "0.0.0.0/0"
    # Adjusts the OOM score for the kube-proxy process
    oomScoreAdj: -999
    # Port range for NodePort services
    portRange: ""
    # Timeout for UDP connections
    udpIdleTimeout: "250ms"
    winkernel:
      # Enable Direct Server Return (DSR) on Windows
      enableDSR: false
      # Name of the network for Windows
      networkName: ""
      # Source VIP for Windows
      sourceVip: ""

# Proxy Mode: The mode line specifies whether kube-proxy should use iptables or ipvs for handling network traffic.
# Cluster CIDR: The clusterCIDR line defines the IP range for the cluster's pod network, which is crucial for routing traffic correctly.
# Kubeconfig Path: The kubeconfig line under clientConnection specifies the path to the kubeconfig file, which contains credentials and connection information for the API server.
# Iptables and IPVS Settings: These sections contain settings specific to the chosen proxy mode, such as synchronization periods and masquerading options.
# Metrics Server: The metricsBindAddress line specifies where the metrics server will listen, which is useful for monitoring.
# NodePort Addresses: The nodePortAddresses line defines which IP addresses can be used for NodePort services, affecting how services are exposed externally.
# OOM Score Adjustment: The oomScoreAdj line adjusts the out-of-memory score for the kube-proxy process, influencing its likelihood of being killed when the system is under memory pressure.
